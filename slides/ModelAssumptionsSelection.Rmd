---
title: "Model Assumptions and Model Selection"
author: "Shravan Vasishth"
date: "6/11/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(lme4)
```

# Goals of this lecture

  - learn to check for the normality of residuals
  - learn to log transform the data
  - learn to compare models to decide which one to use
  - learn carry out your hypothesis test using the likelihood ratio test

# Chinese relative clause data 

```{r}
dat<-read.table("data/gibsonwu2012datarepeat.txt",header=TRUE)
head(dat)
```

```{r}
dat$cond<-ifelse(dat$condition=="subj-ext",-1,1)
m0<-lmer(rt~cond + (1|subj),dat)
summary(m0)

m1<-lmer(rt~cond + (1+cond||subj),dat)
summary(m1)

m2<-lmer(rt~cond + (1+cond|subj),dat)
summary(m2)
```

## Model assumption: residuals are normal

```{r}
hist(residuals(m0))
```

This assumption is clearly violated. A log transform on the reading times will reduce the skew:


```{r}
m0log<-lmer(log(rt)~cond + (1|subj),dat)
m1log<-lmer(log(rt)~cond + (1+cond||subj),dat)
m2log<-lmer(log(rt)~cond + (1+cond|subj),dat)


hist(residuals(m0log))
```

This is good enough for now.

# Model selection

Ignoring model assumptions for a second and analyzing raw rt's, we have three models, m0, m1, m2. Which model is best? There are two schools of thought.

## Barr et al 2013: Always fit the maximal model

Under this view, m2 is always the best model (as long as it converges). If it doesn't converge, then back to the most complex model (m1 or m0) that converges.

See: http://idiom.ucsd.edu/~rlevy/papers/barr-etal-2013-jml.pdf

## The likelihood ratio test (aka analysis of variance or anova)

We compare the ratios of the likelihoods of the two models of interest. For technical reasons, we have to set a value REML to FALSE in the lmer function when doing model comparison (the reason for this is too advanced for this course).


```{r}
m0<-lmer(rt~cond + (1|subj),dat,REML=FALSE)
logLik(m0)

m1<-lmer(rt~cond + (1+cond||subj),dat,REML=FALSE)
logLik(m1)
```

You can see that the ratio is 1. The ratio of the log likelihoods follows a chi-square distribution with the parameter degrees of freedom (df) being the difference in the number of parameters in the two models being compared. In model m0 there are 4 parameters, and in m1 there are 5, so the difference is df=1. So the relevant Chi-sq distribution is chisq(df=1). Let's visualize this and draw the critical chi-sq value (just like the critical t-value in the t-distribution):

```{r}
x<-seq(1,20,by=0.001)
plot(x,dchisq(x,df=1),type="l")
crit_chisq<-qchisq(0.95,df=1)
abline(v=crit_chisq)
```

If the ratio is bigger than the critical chi-squared value (for a given degree of freedom), then we reject the null hypothesis that the two models have the same log likelihoods. If there is no evidence for one model being better, then we choose the simpler model, on grounds of parsimony (Occam's razor). See Bates, Kliegl, Vasishth, Baayen, Parsimonious Mixed Models: https://arxiv.org/abs/1506.04967.

In practice, we can use the anova function (this is literally the likelihood ratio test I showed above) for model comparison:

```{r}
anova(m0,m1)
anova(m1,m2)
anova(m0,m2)
```

Here, m2 is the best model under the likelihood ratio test.

On the log scale, the conclusion is very different!

```{r}
anova(m0log,m1log)
anova(m1log,m2log)
anova(m0log,m2log)
```

m0 is good enough. Since the model assumptions are severely violated in the raw reading time analyses, I would only trust the log rt analyses.


# Checking if a predictor is significant

After you have decided on which model you want to choose as the final one, 
you can now do a significance test to test whether a predictor is statistical significant, using the likelihood ratio test.

Suppose we decide on the m0log model. Then, we can check if relative clauses have an effect as follows. The null hypothesis is that $\beta_1 = 0$.


```{r}
## Null model:
m0logNULL<-lmer(log(rt)~ 1 + (1|subj),dat)
## Alternative model:
m0log<-lmer(log(rt)~1 + cond + (1|subj),dat)
anova(m0logNULL,m0log)

```

If you had gone the Barr et al route, then you would do:


```{r}
m2logNULL<-lmer(log(rt)~1 + (1+cond|subj),dat)
m2log<-lmer(log(rt)~1+cond + (1+cond|subj),dat)
anova(m2logNULL,m2log)
```

In this example, the conclusion is the same. But we will see later that the conclusion can change depending on whether you fit a maximal model or not.

Notice that if I forget to write REML=FALSE when doing model comparison with anova, the software does it automatically. 

# How to report your results in a paper.

``A linear mixed model was fit with \{varying intercepts/varying intercepts and slopes with no correlation/varying intercepts and slopes, the maximal model\}, with SR coded as -1 and OR as +1. The dependent variable (reading time in milliseconds) was log-transformed. The results show that the object relative clause was read faster than the subject relative clause $\chi^2(1)=4.54, p=0.033$.'' 



