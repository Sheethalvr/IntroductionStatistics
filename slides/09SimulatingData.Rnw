\documentclass[slidestop,compress,mathserif,red]{beamer}
%\documentclass[handout]{beamer}

%\usepackage{beamerthemesplit}
\usepackage{fancyvrb}
\usepackage{graphicx}
\usepackage{gb4e}
\usepackage{qtree}
\usepackage{hyperref}
\usepackage{ulem}

\usepackage{amsmath,amssymb,amsfonts}

\setbeamerfont{page number in head/foot}{size=\large}
\setbeamertemplate{footline}[frame number]

%\setbeamertemplate{footline}%
%{%
%\hfill\insertpagenumber\ of \ref{TotPages}\hspace{.5cm}\vspace{.5cm}
%\hfill\insertpagenumber\ of 28\hspace{.5cm}\vspace{.5cm}
%}%



\mode<presentation>
{
%\usetheme{Singapore}
%\usetheme{Berlin}

%\setbeamercovered{transparent}

}


%\mode<handout>
%{
%\usepackage{pgfpages}
%\pgfpagesuselayout{4 on 1}[a4paper,landscape,border shrink=5mm]
%}


\usetheme{Montpellier}
%\usecolortheme{beetle}
%\usecolortheme{seagull}
\usecolortheme{lily}

\title[Lecture 6]{Introduction to statistics: Simulating data}

\author{Shravan Vasishth}

\institute{Universit\"at Potsdam\\
vasishth@uni-potsdam.de\\
http://www.ling.uni-potsdam.de/$\sim$vasishth
}

\date{\today}

\addtobeamertemplate{navigation symbols}{}{ \hspace{1em}    \usebeamerfont{footline}%
    \insertframenumber / \inserttotalframenumber }

\begin{document}
\maketitle



<<setup,include=FALSE,cache=FALSE>>=
library(knitr)
library(coda)

# set global chunk options, put figures into folder
options(replace.assign=TRUE,show.signif.stars=FALSE)
opts_chunk$set(fig.path='figures/figure-', fig.align='center', fig.show='hold')
options(replace.assign=TRUE,width=75)
opts_chunk$set(dev='postscript')
#library(rstan)
#set.seed(9991)
# save workspace image, if you want
#the.date <- format(Sys.time(), "%b%d%Y")
#save.image(file=paste0("homework01-",the.date,".RData")
@



\section{Introduction}

\begin{frame}[fragile]\frametitle{Why simulation  is important}

We will need to  simulate  data to

\begin{itemize}
\item Understand the power properties of our experiment design. This  is important for
\begin{itemize}
\item deciding on sample sizes needed
\item deciding whether we are in a Type M error situation when we get a significant result (can we take the sig.\ result seriously?). 
\end{itemize}
\item Understand which parameters can in principle be recovered under repeated sampling (model selection)
\end{itemize}

\end{frame}

\begin{frame}[fragile]\frametitle{Two simulation scenarios}

We will consider two scenarios:

\begin{itemize}
\item Between-subject designs: This is  just to get you used to simulation. We rarely use this in practice.
\item Within-subject  designs: This  is where all the  interesting action is for us, as we will almost always run repeated measures designs.
\end{itemize}

\end{frame}

\section{Between-subject designs: Simulating  data}

\begin{frame}[fragile]\frametitle{Between-subjects example}

\begin{itemize}
\item
Suppose we have reading time data in milliseconds from 10 subjects who see subject relative clause sentences, and a *different* set of 10 subjects who see object relative clauses. 
\item
Assume that (a) the standard deviation is 300 ms, (b) the true subject relative reading time is 700 ms and (c) the true object relative reading time is 750 ms. 
\item
Thus, the true difference between subject relatives and object relatives is 50 ms. \end{itemize}

How to simulate such data?
\end{frame}

\begin{frame}[fragile]\frametitle{Between-subjects example}


Create a simulated data-set like this:

Take 10 samples from $Normal(\mu=700,\sigma=300)$

Take 10 samples from $Normal(\mu=750,\sigma=300)$

Create a data frame by adding a subject id.
\end{frame}

\begin{frame}[fragile]\frametitle{Between-subjects example}

<<>>=
sr<-rnorm(10,mean=700,sd=300)
or<-rnorm(10,mean=750,sd=300)
subj<-1:20
cond<-rep(c("sr","or"),each=10)
sim_dat<-data.frame(subj=subj,cond=cond,rt=c(sr,or))
head(sim_dat)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Between-subjects example}

Question: How can we repeatedly generate such data?

Answer: write a function (I set some default values here).

<<>>=
betwsubj_simdat<-function(n=10,## no. subjs in each cond.
                          mean1=700,stddev=300,
                          mean2=750){
sr<-rnorm(n,mean=mean1,sd=stddev)
or<-rnorm(n,mean=mean2,sd=stddev)
subj<-1:(2*n)
cond<-factor(rep(c("sr","or"),each=n))
sim_dat<-data.frame(subj=subj,cond=cond,rt=c(sr,or))
sim_dat
}
@

\end{frame}

\begin{frame}[fragile]\frametitle{Between-subjects example}

Now you can repeatedly generate simulated data:

<<>>=
for(i in 1:100){
  sim_dat<-betwsubj_simdat()
}
@

Why would you want to do that? For power analysis, and to check if our model recovers the parameters  correctly under repeated runs of the experiment (with new simulated subjects each time).

\end{frame}

\subsection{Power analysis in a between-subjects design}

\begin{frame}[fragile]\frametitle{Between-subjects example of power analysis}

Analytically:

<<>>=
round(power.t.test(delta=50,n=10,
             sd=300,type="two.sample",
             alternative="two.sided",
             strict=TRUE)$power,3)
@


\end{frame}

\begin{frame}[fragile]\frametitle{Between-subjects example of power analysis}

Using simulation:

<<cache=TRUE>>=
nsim<-100000
## critical t-value:
crit_t<- qt(0.975,df=18)
## observed t-values:
obs_t<-rep(NA,nsim)

for(i in 1:nsim){
sim_dat<-betwsubj_simdat()
obs_t[i]<-t.test(rt~cond,paired=FALSE,sim_dat)$statistic
}
@

\end{frame}

\begin{frame}[fragile]\frametitle{Between-subjects example of power analysis}

<<>>=
pow<-mean(abs(obs_t)>abs(crit_t))
round(pow,2)
@

Lesson learnt: for simple between-subject designs, you can use the analytical approach  or simulation.

\end{frame}

\subsection{Using linear models for between-subjects designs}

\begin{frame}[fragile]\frametitle{Between-subjects example using linear models}

Next, we will learn how to compute power using simulation in \textbf{between}-subjects designs using the linear modeling framework (as opposed to the t.test)

After that, will learn how to compute power using simulation in \textbf{within}-subjects designs using t.test and lmer.

\end{frame}

\begin{frame}[fragile]\frametitle{Between-subjects example using linear models}

Notice that the default contrast coding is 0,1 treatment coding:

<<>>=
sim_dat<-betwsubj_simdat()
contrasts(sim_dat$cond)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Between-subjects example using linear models}

Recode  to sum contrasts:

<<>>=
## Method 1:
contrasts(sim_dat$cond)<-contr.sum(2)
contrasts(sim_dat$cond)
##  Method 2: (my personal preference)
sim_dat$rctyp<-ifelse(sim_dat$cond=="or",1,-1)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Between-subjects example using linear models}

Recall that the two-sample t-test (and ANOVA) and the linear model are exactly the same thing:

<<>>=
##  these are all the same test:
t.test(rt~cond,paired=FALSE,sim_dat)$statistic
@

\end{frame}

\begin{frame}[fragile]\frametitle{Between-subjects example using linear models}

<<>>=
anova(m0<-lm(rt~cond,sim_dat))
@

\end{frame}

\begin{frame}[fragile]\frametitle{Between-subjects example using linear models}

<<>>=
summary(m0<-lm(rt~cond,sim_dat))
@

\end{frame}


\begin{frame}[fragile]\frametitle{Between-subjects example using linear models}

We can write the linear model as follows. 

For every row i in the data frame dat above, the reading time rt is generated from a model where the contrast coding for the condition column in the i-th row of the data is either -1 or +1, depending on whether we are looking at subject or object relatives. $\varepsilon \sim Normal(0,300)$.

\begin{equation}
rt_i = 725 + 25 \times cond_i + \varepsilon_i
\end{equation}

Now, for the first 10 rows of the data frame, we have only SRs, which are coded -1, and for the second 10 rows, we have only ORs, which are coded +1.

\end{frame}

\begin{frame}[fragile]\frametitle{Between-subjects example using linear models}

So, the above definition of the linear model is basically expressing how the reading time rt in each row in the data frame is assumed to be generated:

\tiny
\begin{tabular}{rrr}
row &  Intercept & slope + error \\ 
1 & 725$\times$ 1 & +25$\times$ (-1) + $\varepsilon_1$ \\ 
  2 & 725$\times$ 1 & +25$\times$(-1)  + $\varepsilon_2$ \\ 
  3 & 725$\times$ 1 & +25$\times$(-1) + $\varepsilon_3$\\ 
  4 & 725$\times$ 1 & +25$\times$(-1) + $\varepsilon_4$ \\ 
  5 & 725$\times$ 1 & +25$\times$(-1) + $\varepsilon_5$ \\ 
  6 & 725$\times$ 1 & +25$\times$(-1) + $\varepsilon_6$ \\ 
  7 & 725$\times$ 1 & +25$\times$(-1) + $\varepsilon_7$ \\ 
  8 & 725$\times$ 1 & +25$\times$(-1) + $\varepsilon_8$ \\ 
  9 & 725$\times$ 1 & +25$\times$(-1) + $\varepsilon_9$ \\ 
  10 & 725$\times$ 1 & +25$\times$(-1) + $\varepsilon_{10}$ \\ 
  11 & 725$\times$ 1 & +25$\times$(+1) + $\varepsilon_{11}$ \\ 
  12 & 725$\times$ 1 & +25$\times$(+1) + $\varepsilon_{12}$ \\ 
  13 & 725$\times$ 1 & +25$\times$(+1) + $\varepsilon_{13}$ \\ 
  14 & 725$\times$ 1 & +25$\times$(+1) + $\varepsilon_{14}$ \\ 
  15 & 725$\times$ 1 & +25$\times$(+1) + $\varepsilon_{15}$ \\ 
  16 & 725$\times$ 1 & +25$\times$(+1) + $\varepsilon_{16}$ \\ 
  17 & 725$\times$ 1 & +25$\times$(+1) + $\varepsilon_{17}$ \\ 
  18 & 725$\times$ 1 & +25$\times$(+1) + $\varepsilon_{18}$ \\ 
  19 & 725$\times$ 1 & +25$\times$(+1) + $\varepsilon_{19}$ \\ 
  20 & 725$\times$ 1 & +25$\times$(+1) + $\varepsilon_{20}$ \\ 
\end{tabular}

\end{frame}

\begin{frame}[fragile]\frametitle{Between-subjects example using linear models}

The model m0 is giving us an estimate of the adjustment to the grand mean needed to obtain OR or SR processing cost (700$\pm$ 25). $\varepsilon$ is a random variable with pdf Normal(0,300), and is responsible for the noisiness (variability) in the data.

You can decide whether to reject the null hypothesis in model m0 by looking at the p-value. Recall that the null hypothesis is that OR and SR processing cost has no difference: $H_0: \mu_{SR}-\mu_{OR} = 0$. Here is how you extract the p-value:

<<>>=
summary(m0)$coefficients
## get the second row and fourth column of the output:
summary(m0)$coefficients[2,4]
@

\end{frame}

\begin{frame}[fragile]\frametitle{Between-subjects example using linear models}

You can now do a power analysis of the above experiment design using the linear modeling framework instead of the t-test. Here is how to do it:


<<cache=TRUE>>=
nsim<-10000
pvals<-rep(NA,nsim)
for(i in 1:nsim){
## create random data
## fit linear model
## extract p-value and save in pvals vector
}
## check proportion of times the pvals are less 
## than 0.05. That's your power. 
@

Stop now and try to do this yourself before moving to the next slide.

\end{frame}

\begin{frame}[fragile]\frametitle{Between-subjects example using linear models}


<<cache=TRUE>>=
nsim<-10000
pvals<-rep(NA,nsim)
for(i in 1:nsim){
 sim_dat<-betwsubj_simdat()
 m<-lm(rt~cond,sim_dat)
 pvals[i]<-summary(m)$coefficients[2,4]
}
round(mean(pvals<0.05),2)
@


\end{frame}

\subsection{Checking parameter recover in between-subjects designs}

\begin{frame}[fragile]\frametitle{Between-subjects parameter recovery}

We can also check whether the model can recover the true parameterrs under repeated sampling.

This is an important check of the model's validity; this isn't normally  done  in frequentist courses, but it's a very important and useful tool for model selection, as you will soon see. 

<<cache=TRUE>>=
nsim<-10000
params<-matrix(rep(NA,nsim*3),ncol = 3)

for(i in 1:nsim){
  sim_dat<-betwsubj_simdat()
  m<-lm(rt~cond,sim_dat)
  params[i,c(1,2)]<-summary(m)$coefficients[,1]
  params[i,3]<-summary(m)$sigma
}
@

\end{frame}

\begin{frame}[fragile]\frametitle{Between-subjects parameter recovery}

Parameter recovery is pretty good:

<<fig.height=4,echo=FALSE>>=
op<-par(mfrow=c(1,3),pty="s")
hist(params[,1],main="b0")
abline(v=725)
hist(params[,2],main="b1")
abline(v=25)
hist(params[,3],main="sigma")
abline(v=300)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Between-subjects parameter recovery}

What this good parameter recovery is telling us is that the model can in principle provide accurate estimates  of the true parameter values,  under repeated sampling.

Later you  will see examples of models  that fail to achieve this, for some abstract parameters.

\end{frame}

\section{Within-subjects 1: Simulating data}

\begin{frame}[fragile]\frametitle{Generating within-subjects data}

Now suppose we have a \textbf{total} of 10 subjects, and we show each subject SRs and ORs once each. Now the subject id's become important!

\end{frame}

\begin{frame}[fragile]\frametitle{Generating within-subjects data}

This is how we could try to generate data now:

<<>>=
## number of subjects
n<-10
sr<-round(rnorm(n,mean=700,sd=300))
or<-round(rnorm(n,mean=750,sd=300))
subj<-rep(1:n,2)
cond<-factor(rep(c("sr","or"),each=n))
sim_dat<-data.frame(subj=subj,cond=cond,rt=c(sr,or))
contrasts(sim_dat$cond)<-contr.sum(2)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Generating within-subjects data}

<<>>=
head(sim_dat)
@

\end{frame}
\begin{frame}[fragile]\frametitle{Generating within-subjects data}

Notice that the subject column now repeats the subject id from 1 to n (whatever n is) for SRs and ORs. The total number of subjects is a total of n now, not $2\times n$ like before.

But these generated data do not reflect the fact that we have within-subjects data! The reason: we are incorrectly assuming that SR and OR reading times are independent! The SR and OR data are paired and therefore dependent because they involve the same subjects.

How to induce dependency in the SR and OR data? 

\end{frame}

\begin{frame}[fragile]\frametitle{Generating within-subjects data}


Let us start by assuming that \textbf{each subject has a different true grand mean processing time}. We will use this assumption to induce a dependency.

One way to encode the assumption that each subject has a different processing time is to assume that 

\begin{itemize}
\item some subjects have mean processing time that is greater than the grand mean 725
\item other subjects have a mean processing time that is smaller than 725
\item some subjects have a mean processing time that is the same as 725
\end{itemize}

\end{frame}

\begin{frame}[fragile]\frametitle{Generating within-subjects data}

We can generate random adjustments to the grand mean of 725 by subject by creating a vector of n scores from a random variable that has mean 0 and some standard deviation (I assume 200 here for illustration):

<<>>=
## grand mean processing cost of each subject:
subject_adj<-round(rnorm(n,mean=0,sd=200))
subject_adj
@

Here, if a subject has a value of 0 in the by-subject adjustments, this implies that the true grand mean reading time of the subject is 725 ms.

\end{frame}

\begin{frame}[fragile]\frametitle{Generating within-subjects data}

Now we see that some subjects are faster than 725 ms and some are slower:

<<>>=
round(725+subject_adj)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Generating within-subjects data}

Now we can generate within subjects data as follows:

<<>>=
## create sum coded condition vector:
cond<-rep(c(-1,1),each=n)
rt <- round(725 + rep(subject_adj,2) + 
              25 * cond + rnorm(n*2,0,200))
@

\end{frame}

\begin{frame}[fragile]\frametitle{Generating within-subjects data}

Next, we replace the rt column in our incorrect data frame sim\_dat with this new corrected rt data, which is now coming from a within-subjects design.

<<>>=
## put in new RTs into data frame:
sim_dat$rt<-rt
head(sim_dat)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Generating within-subjects data}

This data can be (in fact, it *must* be) analyzed using the paired t-test!

\end{frame}

\begin{frame}[fragile]\frametitle{Generating within-subjects data}


<<>>=
t.test(rt~cond,sim_dat,paired=TRUE,var.equal=TRUE)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Generating within-subjects data}


Recall that the same model can be fit using the linear mixed model with varying intercepts:

<<>>=
library(lme4)
m1<-lmer(rt~cond+(1|subj),sim_dat,
control=lmerControl(calc.derivs=FALSE))
@

\end{frame}


\begin{frame}[fragile]\frametitle{Generating within-subjects data}

\tiny
<<>>=
summary(m1)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Generating within-subjects data}

The term $(1|subj)$ in the lmer call above refers to the by-subject adjustment to the grand mean that we did above when generating the data.

\end{frame}

\begin{frame}[fragile]\frametitle{Generating within-subjects data}


Now compute power for this within-subjects design using the paired t-test.

Hint: you can keep creating a new vector of data like this, and overwrite the rt column in the data frame sim\_dat:

<<>>=
sim_dat$rt <- round(725 + rep(subject_adj,2) + 
                      25 * cond + 
                      rnorm(2*n,0,200))
@

Then you can do a paired t-test:

\end{frame}

\begin{frame}[fragile]\frametitle{Generating within-subjects data}

<<>>=
t.test(rt~cond,sim_dat,paired=TRUE,var.equal=TRUE)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Generating within-subjects data}


<<cache=TRUE>>=
nsim<-1000
pvals<-tvals_lmer<-rep(NA,nsim)
for(i in 1:nsim){
  subject_adj<-round(rnorm(n,mean=0,sd=200))
  sim_dat$rt <- round(725 + rep(subject_adj,2) + 
                        25 * cond + rnorm(2*n,0,200))
pvals[i]<-t.test(rt~cond,sim_dat,paired=TRUE)$p.value
tvals_lmer<-summary(lmer(rt~cond+(1|subj),sim_dat,
                         control=lmerControl(calc.derivs=FALSE)))$coefficients[2,3]}
## power using paired t-test:
mean(pvals<0.05)
## using lmer:
mean(abs(tvals_lmer)>2)
@

\end{frame}

\subsection{Checking parameter recover in within-subjects designs}

\begin{frame}[fragile]\frametitle{Within-subjects parameter recovery}

Let's write a function for generating repeated measures data:

<<>>=
withinsubj_simdat<-function(n=10,## no. subjs in each condition
                          b0=725,stddev=300,
                          b1=25,sigma_u0=200){
  u0<-round(rnorm(n,mean=0,sd=200))
  cond<-rep(c(-1,1),each=n)
  subj<-rep(1:n,2)
  rt <- round(b0 + rep(u0,2) + 
                        b1 * cond + rnorm(2*n,0,stddev))
sim_dat<-data.frame(subj=subj,cond=cond,rt=rt)
sim_dat
}
@

\end{frame}

\begin{frame}[fragile]\frametitle{Within-subjects parameter recovery}

Now check if the model recovers the four parameters:

<<cache=TRUE>>=
nsim<-10000
params<-matrix(rep(NA,nsim*4),ncol=4)
for(i in 1:nsim){
  sim_dat<-withinsubj_simdat()
  m<-lmer(rt~cond+(1|subj),sim_dat,
          control=lmerControl(calc.derivs=FALSE))
  params[i,1]<-summary(m)$coefficients[1,1]
  params[i,2]<-summary(m)$coefficients[2,1]
  params[i,3]<-sigma_e<-attr(VarCorr(m),"sc")
  params[i,4]<-sigma_e<-attr(VarCorr(m)$subj,"stddev")
}
@

\end{frame}


\begin{frame}[fragile]\frametitle{Within-subjects parameter recovery}

<<fig.height=4,echo=FALSE>>=
op<-par(mfrow=c(2,2),pty="s")
hist(params[,1],main="b0")
abline(v=725)
hist(params[,2],main="b1")
abline(v=25)
hist(params[,3],main="sigma")
abline(v=300)
hist(params[,4],main="sigma_u0")
abline(v=200)
@


\end{frame}

\begin{frame}[fragile]\frametitle{Within-subjects parameter recovery}

So this linear mixed model is looking pretty OK in terms of parameter recovery.

However, in some cases the $\sigma_{u0}$ parameter is not estimated accurately.   

\end{frame}

\section{Within-subjects 2: Simulating data  with multiple data-points for each subject}

\begin{frame}[fragile]\frametitle{Replicates  from each  subject}

Above, we had only one data point from each subject for each RC type. 

<<>>=
xtabs(~subj + cond,sim_dat)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Replicates  from each  subject}

Normally, we collect more than one data point for each condition, from each subject.

We now generate 20 data points from each subject for SRs, and 20 data points from each subject for ORs. The multiple data points from each subjected in each condition is technically called a \textbf{replicate}. 

<<<>>=
## create data frame:
## num subjs
n<-10
## num replicates
k<-20
## the cond vector has to be made n*k times long for each condition:
cond<-rep(c(-1,1),each=n*k)
## subject vector:
## each of the n subject ids has to be repeated k times, and
## there are two conditions, so the whole vector has to be 
## repeated. This can be implemented with two rep functions:
subj<-rep(rep(1:n,each=k),2)
## create data frame:
sim_dat<-data.frame(subj=subj,
                 cond=cond)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Replicates  from each  subject}

<<>>=
head(sim_dat)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Replicates  from each  subject}

<<>>=
## generate repeated measures data:
sim_dat$rt<-725+
            rep(rep(round(rnorm(n,0,200)),each=k),2)+
            25*cond+
            rnorm(n*k*2,0,300)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Replicates  from each  subject}

The only challenging thing here is the adjustment to the intercept:

\begin{itemize}
\item
Each of the n subjects' adjustments has to be repeated k times. 
\item
Then, we need  to repeat that vector of adjustments for SRs and ORs.
\item  
That's how the line of code  below comes about.
\end{itemize}

<<>>=
rep(rep(round(rnorm(n,0,100)),each=k),2)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Replicates  from each  subject}

Break this up step by step to understand it:

<<>>=
## For n=10 subjects, create an adjustment:
u0<-round(rnorm(n,0,100))
@

\end{frame}

\begin{frame}[fragile]\frametitle{Replicates  from each  subject}


Repeat each of these adjustments k=20 times for the first condition:

<<>>=
rep(round(u0),each=k)
@

Now repeat this entire vector for the second condition:

<<>>=
rep(rep(u0,each=k),2)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Replicates  from each  subject}


We now have $10\times20=200$ data points for subject relatives and $200$ for object relatives:

<<>>=
## sanity check time!
dim(sim_dat)
xtabs(~subj+cond,sim_dat)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Replicates  from each  subject}

One can fit a varying intercepts model:

<<>>=
m2<-lmer(rt~cond+(1|subj),sim_dat,
         control=lmerControl(calc.derivs=FALSE))
@

We can also fit a varying intercepts + varying  slopes model:
<<>>=
m3<-lmer(rt~cond+(1+cond|subj),sim_dat,
         control=lmerControl(calc.derivs=FALSE))
@

\end{frame}

\begin{frame}[fragile]\frametitle{Replicates  from each  subject}

You should get output like this (obviously, the numbers will be different for you  as  we are randomly generating data):

<<>>=
summary(m3)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Replicates  from each  subject}
\framesubtitle{By-subject adjustments to mean slope}

Now, we add the assumption that some subjects have larger  relative clause effects, and some smaller effects. Some have the same as same RC effect as the mean RC effect. This generative process corresponds  to  the  varying intercepts and varying slopes model.

Let's assume that subjects variation about the mean slope can be modeled as:

$Normal(0,50)$ 

I just chose an sd of 50 arbitrarily.

<<>>=
(u1<- round(rnorm(n,mean=0,sd=50)))
@

\end{frame}

\begin{frame}[fragile]\frametitle{Replicates  from each  subject}
\framesubtitle{By-subject adjustments to mean slope}

Since the slope in the model is assumed to be 25 ms, what we see is that some subjects have a larger effect and some have a smaller effect than this 25 ms effect.

So, for object relatives:

<<>>=
(25+u1)*1
@

and for subject relatives:

<<>>=
(25+u1)*(-1)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Replicates  from each  subject}
\framesubtitle{By-subject adjustments to mean slope}


We can also plot the variability in the mean SR and and the mean OR effect. In this plot, the circles refer to object relatives, and the square with the cross inside it to subject relatives.

<<>>=
## ORs
plot(1:10,(25+u1)*1,
     xlab="subject",
     ylab="estimate (ms)",ylim=c(-100,100))
## SRs:
points(1:10,(20+u1)*(-1),pch=12)
abline(h=25)
@

We can now generate data that incorporates this additional assumption, that each subject has different slopes:

\end{frame}

\begin{frame}[fragile]\frametitle{Replicates  from each  subject}
\framesubtitle{By-subject adjustments to mean slope}

<<>>=
## generate data for SR and OR conditions row by row:
rt <- 725+rep(rep(round(rnorm(n,0,100)),each=k),2)+
   (25 + rep(rep(round(rnorm(n,0,50)),each=k),2))*cond + 
  rnorm(n*k*2,0,200)
## add to data frame:
sim_dat$rt<-rt
@

\end{frame}

\begin{frame}[fragile]\frametitle{Replicates  from each  subject}
\framesubtitle{By-subject adjustments to mean slope}

We can visualize the RC by-subject effects. First, I  define  a plotting function:

<<>>=
library(ggplot2)
gg_xyplot <- function(x, y, formula, shape, size, 
                      xlabel="RC Type", 
                      ylabel="ms",
                      data=sim_dat){
    ggplot(data = data, aes(x = data[,x],
                            y = data[,y]))  +
    facet_wrap(formula) +
    geom_smooth(method="lm")+
    geom_point(color = "blue", 
               shape = shape, size = size) +
    theme(panel.grid.minor = element_blank()) +
    theme_bw() +
    ylab(ylabel) +
    xlab(xlabel)
}
@

\end{frame}

\begin{frame}[fragile]\frametitle{Replicates  from each  subject}
\framesubtitle{By-subject adjustments to mean slope}

<<echo=FALSE>>=
gg_xyplot(x = "cond", y = "rt",  ~ subj,  
          shape = 1, size = 3, 
          data = sim_dat)
@

Notice that each subject has a different slope.

\end{frame}

\begin{frame}[fragile]\frametitle{Replicates  from each  subject}
\framesubtitle{By-subject adjustments to mean slope}

Fit the following model, which assumes that each subject has a different grand mean (intercept) and different slope. \textbf{Notice that I am using double vertical bars in the code below}. 

<<>>=
m3<-lmer(rt~cond + (1+cond||subj),sim_dat)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Replicates  from each  subject}
\framesubtitle{By-subject adjustments to mean slope}


The output should look something like this (the numbers will vary for each of you).

<<>>=
summary(m3)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Replicates  from each  subject}
\framesubtitle{By-subject adjustments to mean slope}

This model can be written as follows:

$rt \sim Normal(\beta_0 + u_0 + (\beta_1+u_1)*cond,200)$ 

where

$u_0 \sim  Normal(0,100)$

$u_1 \sim  Normal(0,50)$

\end{frame}

\begin{frame}[fragile]\frametitle{Replicates  from each  subject}
\framesubtitle{By-subject adjustments to mean slope}

Notice that there are \textbf{three} variance components:

\begin{itemize}
\item Normal(0,100): the adjustments by subject to the grand mean 
\item Normal(0,50): the adjustments by subjects to the mean slope
\item Normal(0,200): the residual error
\end{itemize}

\end{frame}

\begin{frame}[fragile]\frametitle{Replicates  from each  subject}
\framesubtitle{By-subject adjustments to mean slope}

The model estimates these standard deviations. Here is what I got (the three standard deviations' estimates are shown in order):

\begin{verbatim}
Random effects:
 Groups    Name        Variance Std.Dev.
 subject   (Intercept) 10547    102.7   <- Estimate of u0 sd
 subject.1 cond        12432    111.5   <- Estimate of u1 sd
 Residual              41881    204.6   <- estimate of residual error sd
\end{verbatim}

The first and last estimates are accurate, the slope standard deviation is an overestimate.

\end{frame}

\begin{frame}[fragile]\frametitle{Replicates  from each  subject}
\framesubtitle{A more sophisticated method for generating random slopes}

The data generation approach used above can be made easier to understand by using the following approach: 

<<>>=
## number of subjects
n<-10
## number of replicates
k<-20
## generate data frame with subject and condition column:
cond<-rep(c(-1,1),each=n*k)
sim_dat<-data.frame(sub=rep(rep(1:n,each=k),2),
                 cond=cond)
head(sim_dat)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Replicates  from each  subject}
\framesubtitle{A more sophisticated method for generating random slopes}


Now generate the by-subject intercept adjustments and by subject slope adjustments and create another data frame with these:

<<>>=
sigma_u0<-200
sigma_u1<-50
u<-data.frame(subject=1:n,
              u0=round(rnorm(n,0,sigma_u0)),
              u1=round(rnorm(n,0,sigma_u1)))
@

\end{frame}

\begin{frame}[fragile]\frametitle{Replicates  from each  subject}
\framesubtitle{A more sophisticated method for generating random slopes}

<<>>=
head(u)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Replicates  from each  subject}
\framesubtitle{A more sophisticated method for generating random slopes}

Now, what we will do is that \textbf{for each row} i in the sim\_dat data frame we created above, we will

\begin{itemize}
\item look up the subject id s in the data frame dat's row i
\item find out what the by-subject intercept adjustment (u0) is for that subject s
\item find out what the by-subject slope adjustment (u1) is for that subject s
\item generate the reading time for that row in the data frame sim\_dat using the equation we saw above:
  
  $rt \sim b0 + u0 + (b1 + u1)*cond + rnorm(1,0,300)$ 
  
\end{itemize}
  
\end{frame}

\begin{frame}[fragile]\frametitle{Replicates  from each  subject}
\framesubtitle{A more sophisticated method for generating random slopes}

  
<<>>=
## record the number of rows in the data frame dat:
nrows<-dim(sim_dat)[1]

## create a vector to store reading times generated row by row:
rt<-rep(NA,nrows)

b0<-725
b1<-25
sigma<-300
## run a for-loop to carry out the above steps:
for(i in 1:nrows){
rt[i] <- b0 + u[sim_dat[i,]$subj,]$u0 + 
   (b1 + u[sim_dat[i,]$subj,]$u1)*sim_dat[i,]$cond + rnorm(1,0,sigma)   
}
sim_dat$rt<-rt
@

\end{frame}

\begin{frame}[fragile]\frametitle{Replicates  from each  subject}
\framesubtitle{A more sophisticated method for generating random slopes}

\begin{itemize}
\item
The critical new development here is that in the for loop, we are using the row id to figure out which subject we need to get the adjustment to the intercept and slope for. 
\item
For example, when i=1, i.e., when the row id is 1, we can figure out the subject id in the data frame as follows:

<<>>=
sim_dat[1,]$subj
@
\end{itemize}

\end{frame}

\begin{frame}[fragile]\frametitle{Replicates  from each  subject}
\framesubtitle{A more sophisticated method for generating random slopes}


Once you know this subject id, you can look up appropriate subject id in the data frame u to get the adjustment to the  intercept and  the slope. For example, how  do I  get the  first subject's intercept and slope adjustment?

<<>>=
u[1,]$u0
u[1,]$u1
@

\end{frame}

\begin{frame}[fragile]\frametitle{Replicates  from each  subject}
\framesubtitle{A more sophisticated method for generating random slopes}


Instead of writing 1 manually,  we could have looked up the subject id we need from the i-th row of the data frame sim\_dat! 

<<>>=
u[sim_dat[1,]$subject,]$u0
u[sim_dat[1,]$subject,]$u1
@

That is what the above code that generates the simulated rt data is doing. 

\end{frame}

\begin{frame}[fragile]\frametitle{Replicates  from each  subject}
\framesubtitle{A more sophisticated method for generating random slopes}

\begin{itemize}
\item
And that's it.  Once you have generated the data, fit the same model as m3 to this data. 
\item
Print out the summary of the model, and check whether the model's parameter estimates roughly match the numbers you used to generate the data.
This is just a sanity check for you. 
\end{itemize}

\end{frame}


\begin{frame}[fragile]\frametitle{Replicates  from each  subject}
\framesubtitle{A more sophisticated method for generating random slopes}

Now, using the simpler/more elegant approach I showed above, write a function called gendat that generates data, with input the sample size, the number of repetitions per condition k, the grand mean, the RC effect, and the three standard deviations. 

The function should work as follows. When we type:

\begin{verbatim}
sim_dat<-gendat(n=10,k=20,
            b0=725,b1=25,
            sigma_u0=200,sigma_u1=50,
            sigma=300)
\end{verbatim}

\end{frame}

\begin{frame}[fragile]\frametitle{Replicates  from each  subject}
\framesubtitle{A more sophisticated method for generating random slopes}


We should get the appropriate data frame. For example:

<<echo=FALSE>>=
gendat<-function(n=5,k=5,b0=725,b1=25,
            sigma_u0=200,
            sigma_u1=50,sigma=300){
cond<-rep(c(-1,1),each=n*k)
sim_dat<-data.frame(subj=rep(rep(1:n,each=k),2),
                 cond=cond)

u<-data.frame(subj=1:n,u0=round(rnorm(n,0,sigma_u0)),
                       u1=round(rnorm(n,0,sigma_u1)))

nrows<-dim(sim_dat)[1]

rt<-rep(NA,nrows)

for(i in 1:nrows){
rt[i] <- b0 + u[sim_dat[i,]$subj,]$u0 + 
   (b1 + u[sim_dat[i,]$subj,]$u1)*sim_dat[i,]$cond + rnorm(1,0,sigma)   
}
sim_dat$rt<-rt
sim_dat
}
@

<<>>=
sim_dat<-gendat(n=5,k=5,b0=725,b1=50,
            sigma_u0=200,
       sigma_u1=50,
       sigma=300)
head(sim_dat)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Replicates  from each  subject}
\framesubtitle{A more sophisticated method for generating random slopes}

Using the gendat function, write a for-loop (100 iterations) that computes the proportion of times that the absolute value of the t-score for the slope (the RC effect) is greater than 2. 

This is the estimated power of the experiment; the estimated probability that we would detect the effect (assuming our belief about the true effect is being whatever we set it to be is correct).

<<eval=FALSE,echo=FALSE>>=
tvals<-rep(NA,100)
for(j in 1:100){
  dat<-gendat(samplesize=20,k=20,b0=725,b1=30,
         intsd=150,slopesd=75,residsd=300)
  m0<-lmer(rt~cond+(1+cond||subject),dat)
  tvals[j]<-summary(m0)$coefficients[2,3]
}

mean(abs(tvals)>2)
@

\end{frame}

\section{Within-subjects 3: Simulating data  with multiple data-points for each subject}

\begin{frame}[fragile]\frametitle{Generating data with correlated intercepts and slopes}

Here is how we can generate bivariate correlated data.

Let's assume that the correlation between the varying intercepts and  slopes is 0.5.

<<>>=
library(MASS)
Sigma<-matrix(c(200^2,200*50*0.5,
                200*50*0.5,50^2),ncol=2)

u<-mvrnorm(50,mu=c(0,0),Sigma=Sigma)
head(u)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Generating data with correlated intercepts and slopes}

<<fig.height=4>>=
plot(u[,1],u[,2])
@

\end{frame}

\begin{frame}[fragile]\frametitle{Generating data with correlated intercepts and slopes}

Or assume that the correlation between the varying intercepts and  slopes is -0.5.

<<>>=
Sigma<-matrix(c(200^2,200*50*-0.5,
                200*50*-0.5,50^2),ncol=2)

u<-mvrnorm(50,mu=c(0,0),Sigma=Sigma)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Generating data with correlated intercepts and slopes}

<<fig.height=4>>=
plot(u[,1],u[,2])
@

\end{frame}

\begin{frame}[fragile]\frametitle{Generating data with correlated intercepts and slopes}

Or assume that the correlation between the varying intercepts and  slopes is 0.

<<>>=
Sigma<-matrix(c(200^2,200*50*0,
                200*50*0,50^2),ncol=2)

u<-mvrnorm(50,mu=c(0,0),Sigma=Sigma)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Generating data with uncorrelated intercepts and slopes}

<<fig.height=4>>=
plot(u[,1],u[,2])
@

\end{frame}

\begin{frame}[fragile]\frametitle{Generating data with correlated intercepts and slopes}

Now you should be able to:

\begin{itemize}
\item
extend the gendat function to generate correlated intercepts and slopes
\item 
compute power using this function
\item 
check whether parameters can be  recovered under repeated sampling in the so-called maximal model.
\end{itemize}

These things will be part of the  homework assignments for simulating  data.

\end{frame}


\end{document}