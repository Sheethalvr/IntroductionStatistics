\documentclass[slidestop,compress,mathserif,red]{beamer}
%\documentclass[handout]{beamer}

%\usepackage{beamerthemesplit}
\usepackage{fancyvrb}
\usepackage{graphicx}
\usepackage{gb4e}
\usepackage{qtree}
\usepackage{hyperref}
\usepackage{ulem}

\usepackage{amsmath,amssymb,amsfonts}

\setbeamerfont{page number in head/foot}{size=\large}
\setbeamertemplate{footline}[frame number]

%\setbeamertemplate{footline}%
%{%
%\hfill\insertpagenumber\ of \ref{TotPages}\hspace{.5cm}\vspace{.5cm}
%\hfill\insertpagenumber\ of 28\hspace{.5cm}\vspace{.5cm}
%}%



\mode<presentation>
{
%\usetheme{Singapore}
%\usetheme{Berlin}

%\setbeamercovered{transparent}

}


%\mode<handout>
%{
%\usepackage{pgfpages}
%\pgfpagesuselayout{4 on 1}[a4paper,landscape,border shrink=5mm]
%}


\usetheme{Montpellier}
%\usecolortheme{beetle}
%\usecolortheme{seagull}
\usecolortheme{lily}

\title[Lecture 5]{Introduction to statistics: Linear models}

\author{Shravan Vasishth}

\institute{Universit\"at Potsdam\\
vasishth@uni-potsdam.de\\
http://www.ling.uni-potsdam.de/$\sim$vasishth
}

\date{\today}

\addtobeamertemplate{navigation symbols}{}{ \hspace{1em}    \usebeamerfont{footline}%
    \insertframenumber / \inserttotalframenumber }

\begin{document}
\maketitle



<<setup,include=FALSE,cache=FALSE>>=
library(knitr)
library(coda)

# set global chunk options, put figures into folder
options(replace.assign=TRUE,show.signif.stars=FALSE)
opts_chunk$set(fig.path='figures/figure-', fig.align='center', fig.show='hold')
options(replace.assign=TRUE,width=75)
opts_chunk$set(dev='postscript')
#library(rstan)
#set.seed(9991)
# save workspace image, if you want
#the.date <- format(Sys.time(), "%b%d%Y")
#save.image(file=paste0("homework01-",the.date,".RData")
@



\section{The story so far}

\begin{frame}[fragile]\frametitle{Summary}
\begin{enumerate}
\item We learnt about the single sample, two sample, and paired t-tests.
\item We learnt about Type I, II error (and power).
\item We learnt about Type M and Type S errors.
\end{enumerate}

Now we are ready to look at linear modeling.

\end{frame}

\section{Linear models}

\subsection{Example: Grodner and Gibson relative clause data}

\begin{frame}[fragile]\frametitle{Load Grodner and Gibson  dataset}

<<message=FALSE,eval=FALSE,echo=FALSE>>=
library(dplyr)
gg05e1 <- read.table("data/GrodnerGibson2005E1.csv",
                     sep=",", header=T)
gge1 <- gg05e1 %>% filter(item != 0)

gge1 <- gge1 %>% mutate(word_positionnew = 
                          ifelse(item != 15 & word_position > 10,
                                                  
                                 word_position-1, word_position)) 
gge1crit <- subset(gge1, ( condition == "objgap" & word_position == 6 ) |
            ( condition == "subjgap" & word_position == 4 ))
gge1crit<-gge1crit[,c(1,2,3,6)]
write.table(gge1crit,file="data/grodnergibson05data.txt")
@

<<>>=
gge1crit<-read.table("data/grodnergibson05data.txt",header=TRUE)
head(gge1crit)
@


\end{frame}



\begin{frame}[fragile]\frametitle{Compute means by factor level}

Let's compute the means by factor levels:


<<>>=
means<-round(with(gge1crit,tapply(rawRT,IND=condition,
                            mean)))
means
@

The object relative mean is higher than the subject relative mean.

\end{frame}

\subsection{The paired t-test}

\begin{frame}[fragile]\frametitle{Paired t-test on the data}
\framesubtitle{Correct t-test by subject and by items}


This is how one would do a t-test CORRECTLY with such data, to compare means across conditions:

<<>>=
bysubj<-aggregate(rawRT~subject+condition,
                  mean,data=gge1crit)
byitem<-aggregate(rawRT~item+condition,mean,data=gge1crit)
t.test(rawRT~condition,paired=TRUE,bysubj)$statistic
t.test(rawRT~condition,paired=TRUE,byitem)$statistic
@

<<echo=FALSE,eval=FALSE>>=
library(lme4)
m0lmer<-lmer(rawRT~condition+(1|subject),bysubj)
summary(m0lmer)
@

\end{frame}




\begin{frame}[fragile]\frametitle{Paired t-test on the data}

Consider only by-subject analyses for now.

These are the means we are comparing by subject:

<<>>=
round(with(bysubj,
tapply(rawRT,condition,mean)))
@

\end{frame}

\subsection{Defining the linear model}

\begin{frame}[fragile]\frametitle{Linear models}


We can rewrite our best guess about how the object and subject relative clause reading time distributions like this:

Object relative: $Normal(471,\hat\sigma)$

Subject relative: $Normal(471-102,\hat\sigma)$


\textbf{Note that the two distributions for object and subject relative are assumed to be independent. This is not true in our data as we get a data point each for each RC type from the same subject!}

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}

\begin{itemize}
\item
The object relative's distribution can be written as a sum of two terms:

$y = 471 + \epsilon \hbox{ where } \epsilon \sim Normal(0,\hat\sigma)$
\item
The subject relative's distribution can be written:

$y = 471 - 102 + \epsilon \hbox{ where } \epsilon \sim Normal(0,\hat\sigma)$
\item
Note that $\hat\sigma = 213$ because $obs.t= \frac{\bar{x}}{s/\sqrt{n}} \Rightarrow s = \bar{x} \times \sqrt{n}/obs.t = -103 \times \sqrt{42}/-3.109  =  213$.

<<echo=FALSE,eval=FALSE>>=
obs.t<-t.test(rawRT~condition,paired=TRUE,
               bysubj)$statistic
round(-102*sqrt(42)/obs.t)
@
\end{itemize}

\textbf{The above statements describe a generative process for the data.}

\end{frame}


\begin{frame}[fragile]\frametitle{Linear models}

Now consider this \textbf{linear model}, which describes the rt in each row of the data frame as a function of condition. $\epsilon$ is a random variable $\epsilon \sim Normal(0,213)$.

Object relative reading times:

\begin{equation}
rt = 471 + \epsilon
\end{equation}

Subject relative reading times:

\begin{equation}
rt = 471 - 102 + \epsilon
\end{equation}

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}

When describing mean reading times, I can drop the $\epsilon$:

Object relative reading times:

\begin{equation}
rt = 471
\end{equation}

Subject relative reading times:

\begin{equation}
rt = 471 - 102 
\end{equation}

The lm() function gives us these mean estimates from the data.

\end{frame}


\begin{frame}[fragile]\frametitle{Linear models}

Object relative reading times:

\begin{equation}
rt = 471\mathbf{\times 1} - 102\mathbf{\times 0} + \epsilon
\end{equation}

Subject relative reading times:

\begin{equation}
rt = 471\mathbf{\times 1} - 102\mathbf{\times 1} + \epsilon
\end{equation}

So, object relatives are coded as 0, and subject relatives are coded as 1.

The lm() function sets up such a model.

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}


With real data from the relative clause study:

<<>>=
contrasts(bysubj$condition)
m0<-lm(rawRT~condition,bysubj)
round(summary(m0)$coefficients)[,1]
@

\end{frame}



\begin{frame}[fragile]\frametitle{Linear models}

The linear model gives us two numbers: object relative reading time (471), and the difference between object and subject relative (-102):

<<>>=
round(coef(m0))
@

\end{frame}


\subsection{Contrast coding}

\begin{frame}[fragile]\frametitle{Linear models}

\begin{enumerate}
\item
The \textbf{intercept} is giving us the mean of the objgap condition.
\item 
The \textbf{slope} is giving us the amount by which the subject relative is faster.
\end{enumerate}

Note that the meaning of the intercept and slope depends on the ordering of the factor levels. We can make subject relative means be the intercept:

<<>>=
## reverse the factor level ordering:
bysubj$condition<-factor(bysubj$condition,
                         levels=c("subjgap","objgap"))
contrasts(bysubj$condition)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}

<<>>=
m1a<-lm(rawRT~condition,bysubj)
round(coef(m1a))
@

Now the intercept is the subject relative clause mean. 

The slope is the increase in reading time for the object relative condition.

\end{frame}


\begin{frame}[fragile]\frametitle{Linear models}

<<>>=
## switching back to the original 
## factor level ordering:
bysubj$condition<-factor(bysubj$condition,
                         levels=c("objgap","subjgap"))
contrasts(bysubj$condition)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}

In mathematical form, the model is:

\begin{equation}
rt = \beta_0 + \beta_1 condition + \epsilon
\end{equation}

where 

\begin{itemize}
\item
$\beta_0$ is the mean for the object relative
\item
$\beta_1$ is the amount by which the object relative mean must be 
changed to obtain the mean for the subject relative.
\end{itemize}

The null hypothesis is that the difference in means between the two relative clause types $\beta_1$ is:

$H_0: \beta_1 = 0$

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}

The \textbf{contrast coding} determines the meaning of the $\beta$ parameters:

<<>>=
bysubj$condition<-factor(bysubj$condition,
                         levels=c("objgap","subjgap"))
contrasts(bysubj$condition)
@



\end{frame}


\begin{frame}[fragile]\frametitle{Linear models}

We will make a distinction between the \textbf{unknown true mean} $\beta_0, \beta_1$ and the \textbf{estimated mean from the data} $\hat\beta_0, \hat\beta_1$.

\begin{itemize}
\item
Estimated mean object relative processing time: $\hat\beta_0=\Sexpr{round(coef(m0)[1])}$
.
\item
Estimated mean subject relative processing time: $\hat\beta_0+\hat\beta_1=\Sexpr{round(coef(m0)[1])}+\Sexpr{round(coef(m0)[2])}=\Sexpr{round(coef(m0)[1])+(round(coef(m0)[2]))}$.
\end{itemize}

\end{frame}

\subsection{Sum-contrast coding}

\begin{frame}[fragile]\frametitle{Linear models}
\framesubtitle{Reparameterizing the linear model with sum contrast coding}

In mathematical form, the model is:

\begin{equation}
rt = \beta_0 + \beta_1 condition + \epsilon
\end{equation}

We can change the \textbf{contrast coding} to change the meaning of the $\beta$ parameters:

<<>>=
## new contrast coding:
bysubj$cond<-ifelse(bysubj$condition=="objgap",1,-1)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}
\framesubtitle{Reparameterizing the linear model with sum contrast coding}

<<>>=
xtabs(~cond+condition,bysubj)
@

\end{frame}


\begin{frame}[fragile]\frametitle{Linear models}
\framesubtitle{Reparameterizing the linear model with sum contrast coding}

Now the model parameters have a different meaning:

<<>>=
m1<-lm(rawRT~cond,bysubj)
round(coef(m1))
@

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}
\framesubtitle{Reparameterizing the linear model with sum contrast coding}

\begin{itemize}
\item
Estimated \textbf{grand mean} processing time: $\hat \beta_0=\Sexpr{round(coef(m1)[1])}$.
\item
Estimated mean object relative processing time: $\hat\beta_0+\hat\beta_1=\Sexpr{round(coef(m1)[1])}+\Sexpr{round(coef(m1)[2])}=\Sexpr{round(coef(m1)[1])+round(coef(m1)[2])}$.
\item
Estimated mean subject relative processing time: $\hat\beta_0-\hat\beta_1=\Sexpr{round(coef(m1)[1])}-\Sexpr{round(coef(m1)[2])}=\Sexpr{round(coef(m1)[1])-round(coef(m1)[2])}$.
\end{itemize}

This kind of parameterization is called \textbf{sum-to-zero contrast} or more simply \textbf{sum contrast} coding. This is the coding we will use.

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}
\framesubtitle{The null hypothesis for the slope}

The null hypothesis for the slope is

\begin{equation}
H_0: \mathbf{1\times} \mu_{obj} + (\mathbf{-1\times}) \mu_{subj} = 0   
\end{equation}

The sum contrasts are referring to the $\pm 1$ terms in the null hypothesis:

\begin{itemize}
\item object relative: +1
\item subject relative: -1
\end{itemize}

\end{frame}



\begin{frame}[fragile]\frametitle{Linear models}

Now the model is:

Object relative reading times:

\begin{equation}
rt = 420\mathbf{\times 1} + 51\mathbf{\times 1} + \epsilon
\end{equation}

Subject relative reading times:

\begin{equation}
rt = 420\mathbf{\times 1} + 51\mathbf{\times -1} + \epsilon
\end{equation}

So, object relatives are coded as 1, and subject relatives are coded as -1.

\end{frame}

\subsection{Checking the normality of residuals assumption}

\begin{frame}[fragile]\frametitle{Linear models}

The model is:

\begin{equation}
rt = \beta_0 + \beta_1 + \epsilon \hbox{ where }  \epsilon\sim Normal(0,\sigma)
\end{equation}

It is an assumption of the linear model that the residuals are (approximately) normally distributed.

We can check that this assumption is met:

<<>>=
## residuals:
res.m1<-residuals(m1)
@

\end{frame}


\begin{frame}[fragile]\frametitle{Linear models}

Plot the residuals by comparing them to the standard normal distribution (Normal(0,1)): 

<<echo=FALSE,message=FALSE,fig.height=3.5,message=FALSE>>=
library(car)
qqPlot(res.m1)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}

Another way to visualize it (not the standard way):

<<echo=FALSE,message=FALSE,fig.height=4,message=FALSE>>=
library(car)
x<-seq(-3,3,by=0.001)
plot(x,dnorm(x),type="l")
hist(res.m1/213,freq=FALSE,add=TRUE)
@

\end{frame}


\subsection{Log-transforming the data}

\begin{frame}[fragile]\frametitle{Linear models}
\framesubtitle{Log transformation}

A log-transform improves the normality of residuals:

<<>>=
m1log<-lm(log(rawRT)~cond,bysubj)
round(coef(m1log),4)
@

\end{frame}


\begin{frame}[fragile]\frametitle{Linear models}
\framesubtitle{Log transformation}

\begin{itemize}
\item
Estimated \textbf{grand mean} processing time: $\hat\beta_0=\Sexpr{round(coef(m1log)[1],4)}$
.
\item
Estimated mean object relative processing time: $\hat\beta_0+\hat\beta_1=\Sexpr{round(coef(m1log)[1],4)}+\Sexpr{round(coef(m1log)[2],4)}=\Sexpr{round(coef(m1log)[1],4)+round(coef(m1log)[2],4)}$.
\item
Estimated mean subject relative processing time: $\hat\beta_0-\hat\beta_1=\Sexpr{round(coef(m1log)[1],4)}-\Sexpr{round(coef(m1log)[2],4)}=\Sexpr{round(coef(m1log)[1],4)-round(coef(m1log)[2],4)}$.
\end{itemize}



\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}

The model is:

\begin{equation}
\log rt = \beta_0 + \beta_1  + \epsilon 
\end{equation}
Now check the residuals:

<<>>=
## residuals:
res.m1log<-residuals(m1log)
@

\end{frame}


\begin{frame}[fragile]\frametitle{Linear models}

Plot the residuals by comparing them to the standard normal distribution (Normal(0,1)): 

<<echo=FALSE,message=FALSE,fig.height=3.5>>=
qqPlot(res.m1log)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}

Another way to visualize it (not the standard way):

<<echo=FALSE,message=FALSE,fig.height=4,message=FALSE>>=
library(car)
x<-seq(-3,3,by=0.001)
plot(x,dnorm(x),type="l")
hist(res.m1log/0.404403,freq=FALSE,add=TRUE)
@

\end{frame}


\begin{frame}[fragile]\frametitle{Linear models}
\framesubtitle{Log transformation: recovering estimates on ms scale}

\begin{itemize}
\item
Estimated mean object relative processing time: $\hat\beta_0+\hat\beta_1=\Sexpr{round(coef(m1log)[1],4)}+\Sexpr{round(coef(m1log)[2],4)}=\Sexpr{round(coef(m1log)[1],4)+round(coef(m1log)[2],4)}$.
\item
Estimated mean subject relative processing time: $\hat\beta_0-\hat\beta_1=\Sexpr{round(coef(m1log)[1],4)}-\Sexpr{round(coef(m1log)[2],4)}=\Sexpr{round(coef(m1log)[1],4)-round(coef(m1log)[2],4)}$.
\end{itemize}

Note that $exp(log(rt))=rt$.

To get the mean estimates on the raw ms scale, we just exponentiate both sides of the equation:

$exp(\log rt) = exp( \beta_0 + \beta_1)$ 

\end{frame}


\begin{frame}[fragile]\frametitle{Linear models}
\framesubtitle{Log transformation: recovering estimates on ms scale}

\begin{itemize}
\item
Estimated mean object relative processing time: $exp(\hat\beta_0+\hat\beta_1)=exp(\Sexpr{round(coef(m1log)[1],4)}+\Sexpr{round(coef(m1log)[2],4)})=\Sexpr{round(exp(round(coef(m1log)[1],4)+round(coef(m1log)[2],4)))}$.
\item 
Estimated mean subject relative processing time: $exp(\hat\beta_0-\hat\beta_1)=exp(\Sexpr{round(coef(m1log)[1],4)}-\Sexpr{round(coef(m1log)[2],4)})=\Sexpr{round(exp(round(coef(m1log)[1],4)-round(coef(m1log)[2],4)))}$.
\end{itemize}

The difference in reading time is 417-352=65 ms (cf. 102 ms on raw scale).
\end{frame}

\subsection{Summary}

\begin{frame}[fragile]\frametitle{Linear models: Summary}
\framesubtitle{Summary}

Using the relative clause example, we learnt 

\begin{itemize}
\item the meaning of treatment contrast coding.
\item the meaning of sum contrast coding. \textbf{This is the coding we will use}.
\end{itemize}

For a more comprehensive discussion of contrast coding, read this paper:

\textit{How to capitalize on a priori contrasts in linear (mixed) models: A tutorial},
Schad, Hohenstein, Vasishth, Kliegl. Download from: https://arxiv.org/abs/1807.10451

\end{frame}

\subsection{Revisiting the paired t-test vs the linear model}

\begin{frame}[fragile]\frametitle{Paired t-tests vs linear models}
\framesubtitle{The linear mixed model}

The paired t-test and the linear model t-test values don't match:

<<>>=
t.test(rawRT~condition,bysubj,paired=TRUE)$statistic
round(summary(m0)$coefficients,2)[,c(1:3)]
@

\end{frame}

\begin{frame}[fragile]\frametitle{Paired t-tests vs linear models}
\framesubtitle{The linear mixed model}

This is because the linear model implements the \textbf{unpaired} (i.e., two sample) t-test:

<<>>=
round(summary(m0)$coefficients,2)[,c(1:3)]

round(t.test(rawRT~condition,bysubj,
             paired=FALSE)$statistic,2)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Paired t-tests vs linear models}
\framesubtitle{The linear mixed model}

The paired t-test has an equivalent in the linear modeling framework:

<<include=FALSE,messages=FALSE>>=
library(lme4)
@

<<>>=
m0.lmer<-lmer(rawRT~condition+(1|subject),bysubj)
summary(m0.lmer)$coefficients
@

We turn to the linear mixed model in the next lecture.

\end{frame}


\subsection{Continuous predictors}

\begin{frame}[fragile]\frametitle{Linear models}

\begin{enumerate}
\item
In our relative clause example, the `predictor' is categorial. 
\item
What about when we have continuous predictors?
\item
For example, we have instructors' ``beauty'' levels measured on a continuous scale as predictors of their teaching evaluations.
\item 
Beauty levels are centered; this means that a beauty level of 0 means average beauty level. This is a data set from a paper by 
Hamermesh and Parker
(Beauty in the Classroom: Instructors' Pulchritude and Putative Pedagogical Productivity," Economics of Education Review, August 2005). I got the data from Gelman and Hill (2007).
\end{enumerate}

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}

<<>>=
bdata <- read.table("data/beauty.txt",header=T)
head(bdata)
plot(evaluation~beauty,bdata)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}

Note that the beauty scores are centered to have mean (approximately) 0:

<<>>=
summary(bdata$beauty)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}


One model we can fit:

$y = \beta_0 + \epsilon$

<<>>=
m2<-lm(evaluation~1,bdata)
mean(bdata$evaluation)
round(summary(m2)$coefficients,digits=3)
@

This model is only estimating the grand mean of evaluation scores.

\end{frame}

\begin{frame}[fragile]\frametitle{Linear models}

$y = \beta_0 + \beta_1 x + \epsilon$

<<>>=
m3<-lm(evaluation~beauty,bdata)
round(summary(m3)$coefficients,digits=3)
@

The intercept now means: the expected evaluation score given an average beauty level.

The slope means: the expected increase in evaluation with \textbf{one unit} increase in beauty.

\end{frame}

\section{Summary so far}
\begin{frame}[fragile]\frametitle{Summary}

We now know how to fit

\begin{enumerate}
\item
Simple linear models with a categorical predictor (relative clause data)
\item 
Simple linear models with a continuous predictor.
\end{enumerate}

\end{frame}


\section{Hypothesis testing using the likelihood ratio}

\begin{frame}[fragile]\frametitle{The likelihood ratio test}

Sometimes, students ask me about ANOVA. I generally avoid discussing the ANOVA, but if anyone is interested, here  are the details.

This section is optional. I encourage you to work through it, but  understanding this material is not necessary for the rest of the course. 

\end{frame}

\begin{frame}[fragile]\frametitle{The likelihood ratio test}

Define the likelihood ratio statistic as:

\begin{equation}
\Lambda = \frac{max_{\theta\in \omega_0}(lik(\theta))}{max_{\theta\in \omega_1)}(lik(\theta))}
\end{equation}

\noindent
where, $\omega_0=\{\mu_0\}$ and $\omega_1=\{\forall \mu \mid \mu\neq \mu_0\}$.

Suppose that $X_1,\dots, X_n$ are iid and normally distributed with mean $\mu$ and standard deviation $\sigma$ (assume for simplicity that $\sigma$ is known). 

\end{frame}

\begin{frame}[fragile]\frametitle{The likelihood ratio test}


Let the null hypothesis be $H_0: \mu=\mu_0$ and the alternative be $H_1: \mu\neq \mu_0$. Here, $\mu_0$ is a number, such as $0$.
Now, the numerator of the likelihood ratio statistic is:

\begin{equation}
\frac{1}{(\sigma\sqrt{2\pi})^n} 
           exp\left( -\frac{1}{2\sigma^2} \sum_{i=1}^n (X_i - \mu_0)^2  \right)
\end{equation}

For the denominator, the maximum likelihood can be achieved by specifying the MLE $\bar{X}$ as $\mu$:

\begin{equation}
\frac{1}{(\sigma\sqrt{2\pi})^n} exp \left( -\frac{1}{2\sigma^2} \sum_{i=1}^n (X_i - \bar{X})^2 \right)
\end{equation}

\end{frame}

\begin{frame}[fragile]\frametitle{The likelihood ratio test}


The likelihood ratio statistic is then:

\begin{equation}
\Lambda = 
\frac{\frac{1}{(\sigma\sqrt{2\pi})^n} 
           exp\left( -\frac{1}{2\sigma^2} \sum_{i=1}^n (X_i - \mu_0)^2  \right)}{\frac{1}{(\sigma\sqrt{2\pi})^n} 
           exp\left( -\frac{1}{2\sigma^2} \sum_{i=1}^n (X_i - \bar{X})^2  \right)}
\end{equation}

\end{frame}

\begin{frame}[fragile]\frametitle{The likelihood ratio test}

Canceling out common terms:

\begin{equation}
\Lambda = 
\frac{exp\left( -\frac{1}{2\sigma^2} \sum_{i=1}^n (X_i - \mu_0)^2  \right)}{
        exp\left( -\frac{1}{2\sigma^2} \sum_{i=1}^n (X_i - \bar{X})^2  \right)}
\end{equation}

\end{frame}

\begin{frame}[fragile]\frametitle{The likelihood ratio test}

Taking logs:

\begin{equation}
\begin{split}
\log \Lambda =& 
\left( -\frac{1}{2\sigma^2} \sum_{i=1}^n (X_i - \mu_0)^2  \right)-\left( -\frac{1}{2\sigma^2} \sum_{i=1}^n (X_i - \bar{X})^2  \right)\\
=& -\frac{1}{2\sigma^2} \left( \sum_{i=1}^n (X_i - \mu_0)^2  -  \sum_{i=1}^n (X_i - \bar{X})^2 \right)\\
\end{split}
\end{equation}

\end{frame}

\begin{frame}[fragile]\frametitle{The likelihood ratio test}

Now, note that 

\begin{equation}
\sum_{i=1}^n (X_i -\mu_0)^2 = \sum_{i=1}^n (X_i - \bar{X})^2 + n(\bar{X} - \mu_0)^2 
\end{equation}

\end{frame}

\begin{frame}[fragile]\frametitle{The likelihood ratio test}

This means that 

\begin{equation}
\sum_{i=1}^n (X_i -\mu_0)^2 - \sum_{i=1}^n (X_i - \bar{X})^2 = n(\bar{X} - \mu_0)^2 
\end{equation}

\end{frame}

\begin{frame}[fragile]\frametitle{The likelihood ratio test}


So, we can write $\Lambda$ as:

\begin{equation}
\Lambda = -\frac{1}{2\sigma^2}   n(\bar{X} - \mu_0)^2 
\end{equation}

Rearranging terms:

\begin{equation}
-2 \Lambda =    \frac{n(\bar{X} - \mu_0)^2 }{\sigma^2}
\end{equation}

\end{frame}

\begin{frame}[fragile]\frametitle{The likelihood ratio test}


Or maybe even more transparently:

\begin{equation}
-2 \Lambda =    \frac{(\bar{X} - \mu_0)^2 }{\frac{\sigma^2}{n}}
\end{equation}

This should remind you of the Wald statistic we saw earlier--the t-test! Basically, all this
is saying is that we reject the null when $\mid \bar{X} - \mu_0\mid$ is
large.

\end{frame}

\begin{frame}[fragile]\frametitle{The likelihood ratio test}

More generally, we will define the \textbf{likelihood ratio test statistic} as follows. Here, $L(\theta)$ refers to the likelihood given some value of $\theta$, and 
$\ell(\theta)$ refers to the log likelihood.

\begin{equation}
\begin{split}
L =& -2\times \log (L(\theta_0)/L(\theta_1)) \\
\log L=& -2\times \{\ell (\theta_0)-\ell(\theta_1)\}\\
\log L =& -2 \times \{\ell(\theta_0) - \ell(\theta_1)\}
\end{split}
\end{equation}

where $\theta_1$ and $\theta_0$ are the estimates of $\theta$ under the alternative and null hypotheses, respectively. The likelihood ratio test rejects $H_0$ if $\log L$ is sufficiently large. As the sample size approaches infinity:

\begin{equation}
\log L \rightarrow \chi_r^2  \hbox{ as }  n \rightarrow \infty
\end{equation}

where $r$ is called the degrees of freedom and is the difference in the number of parameters under $H_1$ and $H_0$. 

\end{frame}

\begin{frame}[fragile]\frametitle{The likelihood ratio test}


This is called Wilks' theorem. The proof of Wilks' theorem is fairly involved but you can find it on the internet if you are interested,
or in Lehmann's \textit{Testing Statistical Hypotheses}.

\end{frame}

\begin{frame}[fragile]\frametitle{The likelihood ratio test}

Note that sometimes you will see the form:

\begin{equation}
\log L = 2 \{\ell(\theta_1) - \ell(\theta_0)\}
\end{equation}

It should be clear that both statements are saying the same thing; in the second case, we are just subtracting the null hypothesis log likelihood from the alternative hypothesis log likelihood.

\end{frame}

\begin{frame}[fragile]\frametitle{The likelihood ratio test}

A practical example will make the usage of this test clear.
Let's just simulate a linear model:

<<>>=
x<-1:10
y<- 10 + 2*x+rnorm(10,sd=10)
@

\end{frame}

\begin{frame}[fragile]\frametitle{The likelihood ratio test}

<<simulatelm,fig.height=4>>=
plot(x,y)
@

\end{frame}

\begin{frame}[fragile]\frametitle{The likelihood ratio test}

<<>>=
## null hypothesis model:
m0<-lm(y~1)
## alternative hypothesis model:
m1<-lm(y~x)
@

<<>>=
lambda<- -2*(logLik(m0)-logLik(m1))
## observed value:
lambda[1]
## critical value:
qchisq(0.95,df=1)
# p-value:
pchisq(lambda[1],df=1,lower.tail=FALSE)
@

\end{frame}

\begin{frame}[fragile]\frametitle{The likelihood ratio test}


Here, we fit the null hypothesis model which only has an intercept term $\beta_0$, and the alternative model that has $\beta_1$ as well. 

Finally, we compare the $\lambda$ with the critical chi-squared value for degrees of freedom 1.

We also computed the probability of getting a $\lambda$ as extreme as we got assuming that the null is true:

\end{frame}


\begin{frame}[fragile]\frametitle{The likelihood ratio test}

Note that in the likelihood test above, we are comparing one nested model against another: the null hypothesis model is nested inside the alternative hypothesis model.  

What this means is that the alternative hypothesis model contains all the parameters in the null hypothesis model plus some others.

\end{frame}

\section{Hypothesis testing using Analysis of variance (ANOVA)}

\begin{frame}[fragile]\frametitle{ANOVA}

We can compare two models, one nested inside another, as follows:

<<>>=
anova(m0,m1)
@

\end{frame}

\begin{frame}[fragile]\frametitle{ANOVA}

The F-score you get here is actually the square of the t-value you get in the linear model summary:

<<>>=
sqrt(anova(m0,m1)$F[2])
summary(m1)$coefficients[2,3]
@

This is because $t^2 = F$. The proof is discussed on page 9 of the Dobson and Barnett book.

\end{frame}

\begin{frame}[fragile]\frametitle{ANOVA}


The ANOVA works as follows.  First define the residual as:\footnote{Note that I do not use $\epsilon$ here, but e, to refer to the residual. This is because these are the estimates of $\epsilon$ derived from the estimates $\hat\beta$.}

\begin{equation}
e = Y - X\hat\beta
\end{equation}

The square of this is: 


\begin{equation}
e^T e = (Y - X\hat \beta)^T (Y - X\hat \beta)
\end{equation}

\end{frame}

\begin{frame}[fragile]\frametitle{ANOVA}


Define the \textbf{deviance} as: \label{deviance}

\begin{equation}
\begin{split} \label{eq:deviance}
D =& \frac{1}{\sigma^2} (Y - X\hat \beta)^T (Y - X\hat \beta)\\
=& \frac{1}{\sigma^2}  (Y^T - \hat \beta^TX^T)(Y - X\hat \beta)\\
=& \frac{1}{\sigma^2} (Y^T Y - Y^TX\hat \beta - \hat\beta^TX^T Y + \hat\beta^TX^T  X\hat \beta)\\
%=& \frac{1}{\sigma^2} (Y^T Y - \hat\beta^TX^T Y)\\
\end{split}
\end{equation}

\end{frame}

\begin{frame}[fragile]\frametitle{ANOVA}

Linear  modeling theory tells us $\hat \beta = (X^T X)^{-1} X^T Y$. Premultiplying both sides with $(X^T X)$, we get

$$(X^T X)\hat \beta =  X^T Y$$

It follows that we can rewrite the last line in equation~\ref{eq:deviance} as follows: We can replace $(X^T X)\hat \beta$ with 
$X^T Y$.

\begin{equation}
\begin{split}
D =& \frac{1}{\sigma^2} (Y^T Y - Y^TX\hat \beta - \hat\beta^TX^T Y + \hat\beta^T \underline{X^T  X\hat \beta})\\
=& \frac{1}{\sigma^2} (Y^T Y - Y^TX\hat \beta - \hat\beta^TX^T Y + \hat\beta^T \underline{X^T Y})\\
=& \frac{1}{\sigma^2} (Y^T Y - Y^TX\hat \beta) \\
\end{split}
\end{equation}

\end{frame}

\begin{frame}[fragile]\frametitle{ANOVA}

Notice that $Y^TX\hat \beta$ is a scalar ($1\times 1$) and is identical to  $\beta^TX^T Y$ (check this), so we could write:

<<echo=FALSE>>=
y<-rnorm(10)
X<-matrix(c(rep(1,10),
            rnorm(10)),ncol=2)
b<-solve(t(X) %*% X)%*% t(X)%*% y
#t(y)%*% X %*% b
#t(b)%*%t(X)%*%y
@

\end{frame}

\begin{frame}[fragile]\frametitle{ANOVA}

$$D= \frac{1}{\sigma^2} (Y^T Y - \hat \beta^T X^T Y)$$

Assume now that we have data of size n.

Suppose we have a null hypothesis $H_0: \beta=\beta_0$ and an alternative hypothesis $H_1: \beta=\beta_{1}$. 

Let the null hypothesis have q parameters, and the alternative p, where $q<p<n$.

Let $X_0$ be the design matrix for $H_0$, and $X_1$ the design matrix for $H_1$.

\end{frame}

\begin{frame}[fragile]\frametitle{ANOVA}

Compute the deviances $D_0$ and $D_1$ for each hypothesis, and compute $\Delta D$:

\begin{equation}
\begin{split}
\Delta D =& D_0 - D_1 = \frac{1}{\sigma^2} [(Y^TY - \hat \beta_0 X_0^T Y) -  (Y^TY - \hat \beta_1 X_1^T Y)]\\
=& \frac{1}{\sigma^2} [\hat \beta_1 X_1^T Y - \hat \beta_0 X_0^T Y]\\
\end{split}
\end{equation}

\end{frame}

\begin{frame}[fragile]\frametitle{ANOVA}

It turns out that the F-statistic has the following distribution if the null hypothesis is true:

\begin{equation}
F=\frac{\Delta D/(p-q)}{D_1/(n-p)} \sim F(p-q,n-p)
\end{equation}

So, an extreme value of F is inconsistent with the null and we reject it.

\end{frame}

\begin{frame}[fragile]\frametitle{ANOVA}


The F-statistic is:

\begin{equation}
\begin{split}
F=&\frac{\Delta D/(p-q)}{D_1/(n-p)} \\
=& \frac{\hat \beta_1 X_1^T Y - \hat \beta_0^T X_0^T Y}{p-q} /
\frac{Y^T Y - \hat \beta_1^T X_1^TY}{n-p}\\
\end{split}
\end{equation}

\end{frame}

\begin{frame}[fragile]\frametitle{ANOVA}

Traditionally, the way the F-test is summarized is:

\begin{table}[!htbp]
\caption{ANOVA table}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Source of variance & df & Sum of squares & Mean square\\
\hline
Model with $\beta_0$ & q & $\beta_0^T X_0^T Y$ & \\
Improvement due to & p-q & $\hat \beta_1 X_1^T Y - \hat \beta_0^T X_0^T Y$ & $\frac{\hat \beta_1 X_1^T Y - \hat \beta_0^T X_0^T Y}{p-q}$\\
$\beta_1$ & & & \\
Residual & n-p & $Y^T Y - \hat \beta_1^T X_1^TY$  & $\frac{Y^T Y - \hat \beta_1^T X_1^TY}{n-p}$\\
\hline
Total & n & $y^T y$ & \\
\hline
\end{tabular}
\end{center}
\label{default}
\end{table}%

There is much more to say here about ANOVA, but this is the basic idea.

\end{frame}

\begin{frame}[fragile]\frametitle{ANOVA}

Practically speaking, the usage is simple:

<<>>=
anova(m0,m1)
@

Here, the F-statistic tells us that the model m1 is ``better'' (there  is a significant effect of the predictor x).

Whether that is a meaningful result depends on the power properties of the design (to be discussed later).

\end{frame}

\end{document}


