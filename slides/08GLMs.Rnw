\documentclass[slidestop,compress,mathserif,red]{beamer}
%\documentclass[handout]{beamer}

%\usepackage{beamerthemesplit}
\usepackage{fancyvrb}
\usepackage{graphicx}
\usepackage{gb4e}
\usepackage{qtree}
\usepackage{hyperref}
\usepackage{ulem}

\usepackage{amsmath,amssymb,amsfonts}

\setbeamerfont{page number in head/foot}{size=\large}
\setbeamertemplate{footline}[frame number]

%\setbeamertemplate{footline}%
%{%
%\hfill\insertpagenumber\ of \ref{TotPages}\hspace{.5cm}\vspace{.5cm}
%\hfill\insertpagenumber\ of 28\hspace{.5cm}\vspace{.5cm}
%}%



\mode<presentation>
{
%\usetheme{Singapore}
%\usetheme{Berlin}

%\setbeamercovered{transparent}

}


%\mode<handout>
%{
%\usepackage{pgfpages}
%\pgfpagesuselayout{4 on 1}[a4paper,landscape,border shrink=5mm]
%}


\usetheme{Montpellier}
%\usecolortheme{beetle}
%\usecolortheme{seagull}
\usecolortheme{lily}

\title[Lecture 7]{Introduction to statistics: Generalized linear models (logistic regression)}

\author{Shravan Vasishth}

\institute{Universit\"at Potsdam\\
vasishth@uni-potsdam.de\\
http://www.ling.uni-potsdam.de/$\sim$vasishth
}

\date{\today}

\addtobeamertemplate{navigation symbols}{}{ \hspace{1em}    \usebeamerfont{footline}%
    \insertframenumber / \inserttotalframenumber }

\begin{document}
\maketitle



<<setup,include=FALSE,cache=FALSE>>=
library(knitr)
library(coda)

# set global chunk options, put figures into folder
options(replace.assign=TRUE,show.signif.stars=FALSE)
opts_chunk$set(fig.path='figures/figure-', fig.align='center', fig.show='hold')
options(replace.assign=TRUE,width=75)
opts_chunk$set(dev='postscript')
#library(rstan)
#set.seed(9991)
# save workspace image, if you want
#the.date <- format(Sys.time(), "%b%d%Y")
#save.image(file=paste0("homework01-",the.date,".RData")
@

\section{Introduction}

\begin{frame}[fragile]\frametitle{Logistic regression}

We start with an example data-set that appears in the Dobson et al book: the Beetle dataset.

This data-set shows the number of beetles killed when they were exposed to different doses of some toxic chemical. 

<<>>=
(beetle<-read.table("data/beetle.txt",header=TRUE))
@

\end{frame}

\begin{frame}[fragile]\frametitle{Logistic regression}

The research question is: does dose affect probability of killing insects? The first thing we probably want to do is calculate the proportions:

<<>>=
(beetle$propn.dead<-beetle$killed/beetle$number)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Logistic regression}

It's also reasonable to just plot the relationship between dose and proportion of deaths.

<<echo=FALSE,fig.height=4>>=
with(beetle,plot(dose,propn.dead))
@

\end{frame}

\begin{frame}[fragile]\frametitle{Logistic regression}

Notice that the y-axis is by definition bounded between 0 and 1.

We could easily fit a linear model to this data-set. We may want to center the predictor, for reasons discussed earlier:

<<>>=
fm<-lm(propn.dead~scale(dose,scale=FALSE),beetle)
@

\end{frame}


\begin{frame}[fragile]\frametitle{Logistic regression}

\tiny
<<>>=
summary(fm)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Logistic regression}

<<echo=FALSE,fig.height=4>>=
with(beetle,plot(scale(dose,scale=FALSE),
                 propn.dead))
abline(coef(fm))
@

\end{frame}

\begin{frame}[fragile]\frametitle{Logistic regression}

The interpretation of the coefficients does not in general make sense here because it can land you outside the [0,1] probability range.

Clearly the linear model is failing us. This is the motivation for the generalized linear model. 

\end{frame}

\begin{frame}[fragile]\frametitle{Logistic regression}

Instead of using the linear model, we model log odds instead of proportions $p$ as a function of dose.  Odds are defined as:

\begin{equation}
\frac{p}{1-p}
\end{equation}

and taking the $\log$ will give us log odds.

We are going to model log odds (instead of probability) as a linear function of dose.

\begin{equation}
\log \frac{p}{1-p} = \beta_0 + \beta_1 \hbox{dose}
\end{equation}

The model above is called the logistic regression model. 

\end{frame}

\begin{frame}[fragile]\frametitle{Logistic regression}


Once we have estimated the $\beta$ parameters,  
we can move back from the log odds space to probability space using algebra.

Given a model like

\begin{equation}
\log \frac{p}{1-p} = \beta_0 + \beta_1 \hbox{dose}
\end{equation}

If we exponentiate each side, we get:

\begin{equation}
exp \log \frac{p}{1-p} = \frac{p}{1-p} = exp( \beta_0 + \beta_1 \hbox{dose})
\end{equation}

\end{frame}

\begin{frame}[fragile]\frametitle{Logistic regression}

So now we just solve for p, and get (check this):

\begin{equation} \label{problogisticregression}
p = \frac{exp( \beta_0 + \beta_1 \hbox{dose})}{1+exp( \beta_0 + \beta_1 \hbox{dose})}
\end{equation}

\end{frame}

\begin{frame}[fragile]\frametitle{Logistic regression}


We fit the model in R as follows. Note that as long as I am willing to avoid interpreting the intercept and just interpret the estimate of $\beta_1$, there is no need to center the predictor here:

<<>>=
fm1<-glm(propn.dead~dose,
         binomial(logit),
         weights=number,
         data=beetle)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Logistic regression}

\tiny
<<>>=
summary(fm1)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Logistic regression}


We can also plot the observed proportions and the fitted values together; the fit looks pretty good.

<<propndeadplot,echo=FALSE,fig.height=4>>=
plot(propn.dead~dose,beetle)
points(fm1$fitted~dose,beetle,pch=4)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Logistic regression}

We can now compute the log odds of death for concentration 1.7552 (for example):

<<>>=
## compute log odds of death for 
## concentration 1.7552:
x<-as.matrix(c(1, 1.7552))
#log odds:
(log.odds<-t(x)%*%coef(fm1))
@

\end{frame}

\begin{frame}[fragile]\frametitle{Logistic regression}

We can also obtain the variance-covariance matrix of the fitted coefficients:

<<>>=
### compute CI for log odds:
## Get vcov matrix:
(vcovmat<-vcov(fm1))
## x^T VCOV x for dose 1.7552:
(var.log.odds<-t(x)%*%vcovmat%*%x)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Logistic regression}

And using a normal approximation, we can compute the confidence interval for the 
log odds of death given dose 1.7552:

<<>>=
##lower
(lower<-log.odds-1.96*sqrt(var.log.odds))
##upper
(upper<-log.odds+1.96*sqrt(var.log.odds))
@

\end{frame}

\begin{frame}[fragile]\frametitle{Logistic regression}

The lower and upper confidence interval bounds on the 
probability scale can be computed by 
using equation~\ref{problogisticregression}.

<<>>=
(mean_prob<-exp(log.odds)/(1+exp(log.odds)))
(lower_prob<-exp(lower)/(1+exp(lower)))
(upper_prob<-exp(upper)/(1+exp(upper)))
@

\end{frame}

\begin{frame}[fragile]\frametitle{Logistic regression}

So for dose 1.7552, the probability of death is \Sexpr{round(mean_prob,2)}, with 95\% confidence intervals \Sexpr{round(lower_prob,2)} and \Sexpr{round(upper_prob,2)}.

\end{frame}


\begin{frame}[fragile]\frametitle{Logistic regression}

Note that one should not try to predict outside the range of the design matrix. For example, in the beetle data, the dose ranges from 1.69 to 1.88. We should not try to compute probabilities for dose 2.5, say, since we have no knowledge about whether the relationship remains unchanged beyond the upper bound of our design matrix.

\end{frame}

\section{Multiple logistic regression: Example from Hindi data}

\begin{frame}[fragile]\frametitle{Multiple logistic regression}

\begin{itemize}
\item
We have some Hindi eyetracking data (from Husain et al., 2015). We can compute skipping probability, the probability of skipping a word entirely (i.e., never fixating it). 
\item
The predictors are: word complexity and storage complexity (SC). We expect that the higher the word  complexity and the higher the  storage complexity, the  lower the skipping probability.
\item
We first have to create a vector that has value 1 if the word has 0~ms total reading time, and 0 otherwise. 
\end{itemize}

\end{frame}

\begin{frame}[fragile]\frametitle{Multiple logistic regression}

<<>>=
hindi<-read.table("data/hindiJEMR.txt",header=TRUE)
hindi$skip<-ifelse(hindi$TFT==0,1,0)
fm_skip<-glm(skip ~ word_complex+SC,family=binomial(),hindi)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Multiple logistic regression}
\tiny
<<>>=
summary(fm_skip)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Multiple logistic regression}

The above example also illustrates the second way to set up the data for logistic (multiple) regression: the dependent variable can simply be a 1 or 0 value instead of proportions. So, in the beetle data, you could recode the data to have 1s and 0s instead of proportions. Assuming that you have recoded the column for status (dead or alive after exposure), the glm function call would be:

\begin{verbatim}
glm(dead~dose,family=binomial(),beetle)
\end{verbatim}

Note that logistic regression assumes independence of each data point; this assumption is violated in the Hindi data. For the Hindi data, we will have to use generalized linear mixed models.

\end{frame}

\section{The exponential family of distributions}

\begin{frame}[fragile]\frametitle{The canonical link}

\begin{itemize}
\item
The binomial and normal distributions belong to a wider family of distributions called the exponential family.
\item 
Other examples are: Poisson, Gamma, Probit. 
\end{itemize}

For each of these exponential family distributions, there is a so-called \textbf{canonical link} that gives us the predicted values $x^T \hat\beta$ from the model.

\end{frame}

\begin{frame}[fragile]\frametitle{The canonical link}

For different distributions in the exponential family, the canonical link functions are as follows:

\begin{table}[!htbp]
\centering
\begin{tabular}{|l|l|l|}
\hline
Distribution & $h(x_i^T \beta)=\mu_i$ & Canonical link: $g(\mu_i)=\theta_i$\\
\hline
Binomial & $\frac{exp[\theta_i]}{1+exp[\theta_i]}$ & $\log \frac{y}{1-y}$ \\
logit link & & \\
\hline
Normal & $\theta$ & $g=h$ \\
identity & & \\
\hline
Poisson & $exp[\theta]$ & $\log[\mu]$ \\
log & & \\
\hline
Gamma & $-\frac{1}{\theta}$ & $-\frac{1}{\mu_i}$\\
inverse & & \\
\hline
Cloglog & $1-exp[-exp[\theta_i]]$ & $\log(-\log(1-\mu_i))$\\
cloglog & & \\
\hline
Probit & $\Phi(\theta)$ & 
$\Phi^{-1}(\theta)$ (qnorm)\\
probit & & \\
\hline
\end{tabular}
\end{table}

\end{frame}

\section{Deviance}

\begin{frame}[fragile]\frametitle{Deviance}

Deviance is defined as

\begin{equation}
D = 2[\ell(b_{max}; y) - \ell(b; y)]
\end{equation}

\noindent
where $\ell(b_{max}; y) $ is the log likelihood of the saturated model (the model with the maximal number of parameters that can be fit), and $\ell(b; y) $ is the log likelihood of the model with the parameters b.

Deviance has a chi-squared distribution.

\end{frame}

\begin{frame}[fragile]\frametitle{Deviance for the binomial distribution}

Deviance is defined as $D=\sum d_i$, where:

\begin{equation}
d_i = -2 \times n_i [ y_i \log(\frac{\hat{\mu}_i}{y_i}) + (1-y_i) \log (\frac{1-\hat{\mu}_i}{1-y_i}) ]  
\end{equation}

The basic idea here is that if the model fit is good, Deviance will have a $\chi^2$ distribution with $N-p$ degrees of freedom.

N is the number of data points, p the number of parameters.

So that is what we will use for assessing model fit.

We will also use deviance for hypothesis testing.

\end{frame}


\begin{frame}[fragile]\frametitle{Deviance for the binomial distribution}


The difference in deviance (residual deviance) between two models also has a $\chi^2$ distribution (this should remind you of ANOVA), with dfs being $p-q$, where $q$ is the number of parameters in the first model, and $p$ the number of parameters in the second.

I discuss hypothesis testing first, then evaluating goodness of fit using deviance.

\end{frame}
%Note that $\sum e_{D,i} = D$.

\subsection{Hypothesis testing: Residual deviance}

\begin{frame}[fragile]\frametitle{Deviance for the binomial distribution}

Returning to our beetle data, let's say we fit our model:

<<>>=
glm1<-glm(propn.dead~dose,binomial(logit),
          weights=number,data=beetle)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Deviance for the binomial distribution}

The summary output shows us the number of iterations that led to the parameter estimates:
\tiny
<<>>=
summary(glm1)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Deviance for the binomial distribution}

But we also see something called \textbf{Null deviance} and \textbf{Residual deviance}. These are used to evaluate quality of model fit. Recall that we can compute the fitted values and compare them to the observed values:

<<propndead2,fig.width=6>>=
# beta.hat is (-60.71745 ,   34.27033)
(eta.hat<-  -60.71745 +   34.27033*beetle$dose)
(mu.hat<-exp(eta.hat)/(1+exp(eta.hat)))
@

\end{frame}

\begin{frame}[fragile]\frametitle{Deviance for the binomial distribution}

<<fig.height=4>>=
# compare mu.hat with observed proportions
plot(mu.hat,beetle$propn.dead)
abline(0,1)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Deviance for the binomial distribution}

To evaluate whether dose has an effect, we will do something analogous to the model comparison methods we saw earlier. First, fit a model with only an intercept. Notice that the null deviance is 284 on 7 degrees of freedom.

<<propndead3,fig.width=6>>=
null.glm<-glm(propn.dead~1,binomial(logit),
          weights=number,data=beetle)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Deviance for the binomial distribution}
\tiny
<<>>=
summary(null.glm)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Deviance for the binomial distribution}

<<echo=FALSE,fig.height=4>>=
plot(beetle$dose,beetle$propn.dead,xlab="log concentration",
    ylab="proportion dead",main="minimal model")
points(beetle$dose,null.glm$fitted,pch=4)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Deviance for the binomial distribution}

Add a term for dose. Now, the residual deviance is 11.2 on 6 dfs.

<<propndead4,fig.width=6>>=
dose.glm<-glm(propn.dead~dose,binomial(logit),
          weights=number,data=beetle)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Deviance for the binomial distribution}
\tiny
<<>>=
summary(dose.glm)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Deviance for the binomial distribution}


<<echo=FALSE,fig.height=4>>=
plot(beetle$dose,beetle$propn.dead,xlab="log concentration",
    ylab="proportion dead",main="dose model")
points(beetle$dose,dose.glm$fitted,pch=4)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Deviance for the binomial distribution}

The change in deviance from the null model is 284.2-11.2=273 on 1 df. Since the critical $\chi_1^2 = 3.84$, we reject the null hypothesis that $\beta_1 = 0$.

You can do the model comparison using the anova function. Note that no statistical test is calculated; you need to do that yourself.
\tiny
<<>>=
anova(null.glm,dose.glm)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Deviance for the binomial distribution}

Actually, you don't even need to define the null model; the anova function automatically compares the fitted model to the null model:

\tiny
<<>>=
anova(dose.glm)
@

\end{frame}

\section{Assessing goodness of fit of a fitted model}

\begin{frame}[fragile]\frametitle{Goodness of fit}

The deviance for a given degrees of freedom $v$ should have a $\chi_v^2$ distribution for the model to be adequate. As an example, consider the null model above. The deviance is clearly much larger than the 95th percentile cutoff point of the chi-squared distribution with 7 dfs, so the model is not adequate.

<<>>=
deviance(null.glm)
## critical value:
qchisq(0.95,df=7)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Goodness of fit}

Now consider the model with dose as predictor. The deviance is less than the 95th percentile, so the fit is adequate.

<<>>=
deviance(dose.glm)
qchisq(0.95,df=6)
@

\end{frame}



\section{Generalized linear mixed models}

\begin{frame}[fragile]\frametitle{Generalized linear mixed models}

The GLMM is now straightforward: we know the basic theory of linear mixed models already, and the syntax is very general.

<<>>=
## varying intercepts model:
library(lme4)
fm_skip_lmer<-glmer(skip ~ word_complex+SC
                    +(1|subj)+(1|item),
               family=binomial(),hindi)
@
\end{frame}

\begin{frame}[fragile]\frametitle{Generalized linear mixed models}
\tiny
<<>>=
summary(fm_skip_lmer)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Generalized linear mixed models}

Centering the predictors yields lower correlations of fixed effects:

<<>>=
fm_skip_lmer2<-glmer(skip ~ scale(word_complex,
                                  scale=FALSE)+
                      scale(SC,scale=FALSE) 
                     +(1|subj)+(1|item),
               family=binomial(),hindi)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Generalized linear mixed models}


\tiny
<<>>=
summary(fm_skip_lmer2)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Generalized linear mixed models}

Model comparison is done the same way as before, using anova().

<<>>=
fm_skip_lmer2NULL<-glmer(skip ~ 
                      scale(SC,scale=FALSE) 
                     +(1|subj)+(1|item),
               family=binomial(),hindi)
@


\end{frame}

\begin{frame}[fragile]\frametitle{Generalized linear mixed models}

Model comparison is done the same way as before, using anova().
\tiny
<<>>=
## the effect of word complexity
anova(fm_skip_lmer2NULL,fm_skip_lmer2)
@
\normalsize
Caveat: this model is overly simple, it has no varying slopes. A proper investigation would have to include varying slopes for both predictors.

\end{frame}

\end{document}
