\documentclass[slidestop,compress,mathserif,red]{beamer}
%\documentclass[handout]{beamer}

%\usepackage{beamerthemesplit}
\usepackage{fancyvrb}
\usepackage{graphicx}
\usepackage{gb4e}
\usepackage{qtree}
\usepackage{hyperref}
\usepackage{ulem}

\usepackage{amsmath,amssymb,amsfonts}

\setbeamerfont{page number in head/foot}{size=\large}
\setbeamertemplate{footline}[frame number]

%\setbeamertemplate{footline}%
%{%
%\hfill\insertpagenumber\ of \ref{TotPages}\hspace{.5cm}\vspace{.5cm}
%\hfill\insertpagenumber\ of 28\hspace{.5cm}\vspace{.5cm}
%}%



\mode<presentation>
{
%\usetheme{Singapore}
%\usetheme{Berlin}

%\setbeamercovered{transparent}

}


%\mode<handout>
%{
%\usepackage{pgfpages}
%\pgfpagesuselayout{4 on 1}[a4paper,landscape,border shrink=5mm]
%}

\usetheme{Montpellier}
%\usecolortheme{beetle}
%\usecolortheme{seagull}
\usecolortheme{lily}

\title[Lecture 3]{Introduction to statistics: Calibrating errors and the t-test continued}

\author{Shravan Vasishth}

\institute{Universit\"at Potsdam\\
vasishth@uni-potsdam.de\\
http://www.ling.uni-potsdam.de/$\sim$vasishth
}

\date{\today}

\addtobeamertemplate{navigation symbols}{}{ \hspace{1em}    \usebeamerfont{footline}%
    \insertframenumber / \inserttotalframenumber }


\begin{document}
\maketitle



<<setup,include=FALSE,cache=FALSE>>=
library(knitr)
library(coda)

# set global chunk options, put figures into folder
options(replace.assign=TRUE,show.signif.stars=FALSE)
opts_chunk$set(fig.path='figures/figure-', fig.align='center', fig.show='hold')
options(replace.assign=TRUE,width=75)
opts_chunk$set(dev='postscript')

@

\section{Type I, Type II, power}


\begin{frame}[fragile]\frametitle{Type I, Type II error, power}

 When we do a hypothesis test, the sample mean 
\begin{enumerate}
\item
will either fall in the rejection region $\rightarrow$ reject null
\item 
or it will not  $\rightarrow$ fail to reject null
\end{enumerate}

But the null hypothesis is either true or not true.
\textit{We don't know which of those two is the reality}.

\end{frame}
\begin{frame}[fragile]\frametitle{Type I, Type II error, power}

\begin{tabular}{ccc}
\hline
Reality: & $H_0$ TRUE & $H_0$ FALSE \\
\hline
Decision: `reject': & $\alpha$ & $1~-~\beta$ \\
                                     & \textbf{Type I error}                         & \textbf{Power} \\                                      
                                     & & \\
\hline
Decision: `fail to reject': & $1 - \alpha$ & $\beta$ \\                                    &                                 & \textbf{Type II error}\\
\hline
\end{tabular}

\bigskip

\textbf{For simplicity, assume that SE=1}. This means that the t-score is really the sample mean.

Consider the situation where, in reality, the true $\mu=2$ and the null hypothesis $H_0$ is that $\mu=0$. Now the $H_0$ is false.


\end{frame}

\begin{frame}[fragile]\frametitle{Type I, Type II error, Power}

Type I error is conventionally held at 0.05. Power is 1-Type II error.

<<echo=FALSE,fig.height=4>>=
## function for plotting AUC:
plot.prob<-function(x,
                           x.min,
                           x.max,
                           prob,
                           mean,
                           sd,
                           gray.level,main){

        plot(x,dnorm(x,mean,sd), 
                     type = "l",xlab="",
             ylab="",main=main)
        abline(h = 0)

## shade X<x    
    x1 = seq(x.min, qnorm(prob), abs(prob)/5)
    y1 = dnorm(x1, mean, sd)

    polygon(c(x1, rev(x1)), 
            c(rep(0, length(x1)), rev(y1)), 
            col = gray.level)
  }

shadenormal<- 
function (prob=0.5,
          gray1=gray(0.3),
          x.min=-6,
          x.max=abs(x.min),
          x = seq(x.min, x.max, 0.01),
          mean=0,
          sd=1,main="P(X<0)") 
{

plot.prob(x=x,x.min=x.min,x.max=x.max,
               prob=prob,
                      mean=mean,sd=sd,
     gray.level=gray1,main=main)     
}

shadenormal(prob=0.025,main="Type I, II error")

x1 <- seq(qnorm(0.975),6,abs(0.975)/5)
y1 <- dnorm(x1)

polygon(c(x1, rev(x1)), 
        c(rep(0, length(x1)), rev(y1)), 
        col = gray(0.3))

x<-seq(-6,6,by=0.1)
lines(x,dnorm(x,mean=2),col="red",lwd=2)
abline(v=2)
abline(v=-2)

x1 <- seq(-2,2,0.01)
y1 <- dnorm(x1,mean=2)

polygon(c(x1, rev(x1)), 
        c(rep(0, length(x1)), rev(y1)), 
        col = "orange")

@

\end{frame}

\subsection{Example}

\begin{frame}[fragile]\frametitle{The typical statistical test}
\framesubtitle{t-test}

Given data:

<<>>=
## Sampling from Normal(0,1)
(sample<-rnorm(10))
@

\end{frame}

\begin{frame}[fragile]\frametitle{The typical statistical test}
\framesubtitle{t-test}


If we do a t-test to test the null hypothesis that $\mu = 0$:

<<>>=
n<-length(sample)
x_bar<-mean(sample)
stddev<-sd(sample)
(t_value<- (x_bar - 0)/(stddev/sqrt(n)))
@

\end{frame}

\begin{frame}[fragile]\frametitle{The typical statistical test}
\framesubtitle{t-test}

We can also compute the probability of getting the observed t-value that we got from the sample, or something more extreme, given the null hypothesis.

This can be computed, as done earlier, simply by calculating the area under the curve in the rejection region for the relevant t-distribution:

<<>>=
pt(-abs(t_value),df=n-1)
@

\end{frame}

\begin{frame}[fragile]\frametitle{The typical statistical test}
\framesubtitle{t-test}


I just took the absolute t-value from the sample and took its negation in order to compute the probability on the left tail. I could have also written:

<<>>=
pt(abs(t_value),df=n-1,lower.tail=FALSE)
@


\end{frame}

\begin{frame}[fragile]\frametitle{The typical statistical test}
\framesubtitle{t-test}

The convention is to compute the probability of getting the observed t-value that we got or something more extreme on either side of the t-distribution --- we look at both sides of the t-distribution.

<<>>=
2*pt(-abs(t_value),df=n-1)
@

Conventionally, we reject the null if $p<0.05$. This is because we set the Type I error at 0.05.

\end{frame}


\begin{frame}[fragile]\frametitle{The typical statistical test}

<<>>=
2*pt(-abs(t_value),df=n-1)
@

Conventionally, we reject the null if this probability $<0.05$. 

This probability is called the p-value.

\end{frame}

\begin{frame}[fragile]\frametitle{The typical statistical test}
\framesubtitle{t-test}

You can use the built-in function in R to do such a t-test:

<<>>=
t.test(sample)
@

\end{frame}




\begin{frame}[fragile]\frametitle{Type I error vs p-values}

Type I error is the False Discovery Rate: the probability of your incorrectly rejecting the null under repeated sampling: 
<<cache=TRUE>>=
nsim<-10000
n<-10
pvals<-rep(NA,nsim)
for(i in 1:nsim){
  x<-rnorm(n)
  pvals[i]<-t.test(x)$p.value
}
mean(pvals<0.05)
@

The p-value you get from one experiment will vary from experiment to experiment. Type I error is a value we fix in advance.

\end{frame}

\begin{frame}[fragile]\frametitle{Type II error (1-power) vs p-values}

\begin{enumerate}
\item Many studies in linguistics and psychology have very low power (sometimes as low as 0.06). (1-power) is Type II error.
\item This implies that if power is low and we get a so-called null result, i.e., fail to reject the null hypothesis (when $p>0.05$), we can't really conclude anything. \item If power were high, then a null result could be more meaningful and we might be justified in accepting the null.
\end{enumerate}

But the situation with low power is not just that null results are inconclusive. Even ``statistically significant'' results are suspect with low power.

\end{frame}

\subsection{Type S and Type M errors}

\begin{frame}[fragile]\frametitle{Type S- and M-error}
\framesubtitle{Gelman and Carlin, Beyond Power Calculations:
Assessing Type S (Sign) and Type M (Magnitude) Errors, Perspectives on Psychological Science November 2014 vol.\ 9 no.\ 6 641-651}

If your true effect size is believed to be $D=15$, 
then we can compute (apart from statistical power) these error rates, which are defined as follows:

\begin{enumerate}
\item
\textbf{Type S error}: the probability that the sign of the effect is incorrect, given that the result is statistically significant.
\item 
\textbf{Type M error}: the expectation of the ratio of the absolute magnitude of the effect to the hypothesized true effect size, given that result is significant. 
Gelman and Carlin also call this the exaggeration ratio, which is perhaps more descriptive than ``Type M error''.
\end{enumerate}


\end{frame}

\begin{frame}[fragile]\frametitle{Type S- and M-error}

Suppose a particular study has standard error 46, and sample size 37. And suppose that our true $\mu=15$. Then, we can compute Type S error as follows:

<<typesandm,cache=TRUE,echo=TRUE>>=
## probable effect size, derived from past studies:
D<-15
## SE from the study of interest:
se<-46
stddev<-se*sqrt(37)
nsim<-10000
drep<-rep(NA,nsim)
for(i in 1:nsim){
samp<-rnorm(37,mean=D,sd=stddev)
drep[i]<-mean(samp)
}
@
\end{frame}

\begin{frame}[fragile]\frametitle{Type S- and M-error}

<<typesandm2,cache=TRUE,echo=TRUE>>=
##power: the proportion of cases where 
## we reject the null hypothesis correctly:
(pow<-mean(ifelse(abs(drep/se)>2,1,0)))
@

\end{frame}

\begin{frame}[fragile]\frametitle{Type S- and M-error}

<<typesandm3,cache=TRUE,echo=TRUE>>=
## which cells in drep are significant at alpha=0.05?
signif<-which(abs(drep/se)>2)

## Type S error rate | signif:
(types_sig<-mean(drep[signif]<0))
@

\end{frame}

\begin{frame}[fragile]\frametitle{Type S- and M-error}

<<typesandm4,cache=TRUE,echo=TRUE>>=
## Type M error rate | signif: 
(typem_sig<-mean(abs(drep[signif])/D))
@

\end{frame}

\begin{frame}[fragile]\frametitle{Type S- and M-error}
\framesubtitle{Funnel plot}

<<funnelplot,echo=FALSE,fig.height=4,cache=TRUE>>=
## funnel plot:
truemu<-15
se<-46
sampsize<-seq(10,10000,by=1)
n_expts<-length(sampsize)
means<-power<-rep(NA,n_expts)
SE<-46
for(i in 1:n_expts){
  x<-rnorm(sampsize[i],mean=truemu,
                       sd=se)
  means[i]<-mean(x)
  power[i]<-power.t.test(d=truemu,sd=250,
                      n=sampsize[i])$power
}

plot(means,power,
     main="Funnel plot",
     xlim=range(c(min(means),max(means))),
     xlab="effect",ylab="power",
     #ylim=c(0,0.003),
     cex.lab=1.8,
     cex.axis=1.5,cex.main=1.5)
abline(v=15)
abline(h=0.20,col="red")
@

\end{frame}

\begin{frame}[fragile]\frametitle{Type S- and M-errors}

\begin{enumerate}
\item
So, you can see that the Type S error and the exaggeration ratio, conditional on a result being significant, are pretty high. 
\item
The practical implication of this is that if most studies are low powered, then \textbf{it doesn't matter much whether you got a significant result or not}. 
\item
The main point here is: run \textbf{high powered} studies, and \textbf{replicate} the results. There's really nothing that can match consistent replication.
\end{enumerate}

\end{frame}

\subsection{Stopping rules}

\begin{frame}[fragile]\frametitle{Stopping rules}

Researchers often adopt the following type of data-gathering procedure:

\begin{enumerate}
\item
The experimenter gathers $n$ data points, then checks for significance ($p<0.05$ or not). 
\item
If it's not significant, he gets more data ($n$ more data points). Since time and money are limited, he might decide to stop anyway at sample size, say, some multiple of $n$. 
\end{enumerate}

\end{frame}

\begin{frame}[fragile]\frametitle{Stopping rules}

\begin{enumerate}
\item
One can play with different scenarios here. A typical $n$ might be $15$.
\item
This approach would give us a range of p-values under repeated sampling. 
\item
Theoretically, under the standard assumptions of frequentist methods, we expect a Type I error to be $0.05$. This is the case in standard analyses (I also track the t-statistic, in order to compare it with my stopping rule code below).
\end{enumerate}

\end{frame}

\begin{frame}[fragile]\frametitle{Stopping rules}

<<stoppingrules>>=
##Standard:
pvals<-NULL
tstat_standard<-NULL
n<-10
nsim<-10000
## assume a standard dev of 1:
stddev<-1
mn<-0
for(i in 1:nsim){
  samp<-rnorm(n,mean=mn,sd=stddev)
  pvals[i]<-t.test(samp)$p.value
  tstat_standard[i]<-t.test(samp)$statistic
}

@
\end{frame}

\begin{frame}[fragile]\frametitle{Stopping rules}

<<>>=
## Type I error rate: about 5% as theory says:
table(pvals<0.05)[2]/nsim
@

\end{frame}

\begin{frame}[fragile]\frametitle{Stopping rules}

But the situation quickly deteriorates as soon as adopt the strategy I outlined above. I will also track the distribution of the t-statistic below.

<<stoppingrules2>>=
pvals<-NULL
tstat<-NULL
## how many subjects can I run?
upper_bound<-n*6
@

\end{frame}

\begin{frame}[fragile]\frametitle{Stopping rules}

\small
<<>>=
for(i in 1:nsim){
  significant<-FALSE 
  x<-rnorm(n,mean=mn,sd=stddev) ## take sample
while(!significant & length(x)<upper_bound){
  ## if not significant:
if(t.test(x)$p.value>0.05){
  x<-append(x,rnorm(n,mean=mn,sd=stddev)) ## get more data
} else {significant<-TRUE}   ## otherwise stop:
}
pvals[i]<-t.test(x)$p.value
tstat[i]<-t.test(x)$statistic
}
@

\end{frame}

\begin{frame}[fragile]\frametitle{Stopping rules}

<<>>=
## Type I error rate: much higher than 5%:
table(pvals<0.05)[2]/nsim
@

\end{frame}

\begin{frame}[fragile]\frametitle{Stopping rules}

Now let's compare the distribution of the t-statistic in the standard case vs with the above stopping rule (red):

<<stoppingrules3,echo=FALSE,fig.height=4>>=
plot(density(tstat_standard),main="",xlab="")
lines(density(tstat),col="red")
#hist(tstat_standard,
#main="The t-distributions for the standard case (white) \n
#     vs the stopping rule (gray)",freq=F)
#hist(tstat,add=T,col="#0000ff22",freq=F)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Stopping rules}

\begin{enumerate}
\item
We get fatter tails with the above stopping rule---a higher Type I error than 0.05.
\item
The point is that one should fix one's sample size in advance based on a power analysis, not deploy a stopping rule like the one above; if we used such a stopping rule, we are much more likely to incorrectly declare a result as statistically significant.
\item
Of course, if your goal is to get an article published no matter what, such stopping rules are a great way to have a successful career!
\end{enumerate}

\end{frame}

\section{Summary so far}

\begin{frame}[fragile]\frametitle{Summary}
\begin{enumerate}
\item We learnt about the single sample t-test.
\item We learnt about Type I, II error (and power).
\item We learnt about Type M and Type S errors.
\end{enumerate}

\end{frame}

\end{document}

